{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Syswea/machine-learning-demo/blob/main/%E2%80%9Ctransformer_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "31cc22e1",
      "metadata": {
        "id": "31cc22e1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "import numpy as np\n",
        "import baostock as bs\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mplfinance.original_flavor import candlestick_ohlc\n",
        "import matplotlib.dates as mdates\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.dates import DateFormatter, DayLocator, date2num\n",
        "# pip install baostock mplfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "357a1182",
      "metadata": {
        "id": "357a1182"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "LOOKBACK = 30\n",
        "FORECAST = 7\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "NUM_LAYERS = 2\n",
        "NUM_HEADS = 2\n",
        "D_MODEL = 256\n",
        "D_FF = 512\n",
        "DROPOUT = 0.5\n",
        "LR = 0.01\n",
        "EPOCHS = 20\n",
        "PATIENCE = 5\n",
        "\n",
        "val_coef = 0.1\n",
        "vaild_coef = 2\n",
        "vol_coef = 3\n",
        "rate_coef = 500\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.random.manual_seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "34d9a0f0",
      "metadata": {
        "id": "34d9a0f0"
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, num_features, d_model):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.linear = nn.Linear(num_features, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "f0bb8fcb",
      "metadata": {
        "id": "f0bb8fcb"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        encoding = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        encoding = encoding.unsqueeze(0)\n",
        "\n",
        "        # Register the encoding as a buffer so it is not a model parameter\n",
        "        self.register_buffer('encoding', encoding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.encoding[:, :x.size(1), :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "7296ebca",
      "metadata": {
        "id": "7296ebca"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Q的形状: [batch_size, num_heads, seq_len, d_k]\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # 如果提供了mask，则将其应用于注意力分数\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # 应用softmax获取注意力权重\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "        attn_weights = self.drop(attn_weights)\n",
        "\n",
        "        # 应用注意力权重到值(V)\n",
        "        output = torch.matmul(attn_weights, V)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # 输入x的形状: [batch_size, seq_len, d_model]\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "\n",
        "        # 重塑为 [batch_size, seq_len, num_heads, d_k]\n",
        "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
        "\n",
        "        # 转置为 [batch_size, num_heads, seq_len, d_k]\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # 输入x的形状: [batch_size, num_heads, seq_len, d_k]\n",
        "        batch_size, _, seq_len = x.size(0), x.size(1), x.size(2)\n",
        "\n",
        "        # 转置回 [batch_size, seq_len, num_heads, d_k]\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # 重塑为 [batch_size, seq_len, d_model]\n",
        "        return x.contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # 线性变换\n",
        "        Q = self.W_q(query)\n",
        "        K = self.W_k(key)\n",
        "        V = self.W_v(value)\n",
        "\n",
        "        # 分割头\n",
        "        Q = self.split_heads(Q)\n",
        "        K = self.split_heads(K)\n",
        "        V = self.split_heads(V)\n",
        "\n",
        "        # 应用缩放点积注意力\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # 合并头\n",
        "        output = self.combine_heads(attn_output)\n",
        "\n",
        "        # 最终线性变换\n",
        "        output = self.W_o(output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "c30e0479",
      "metadata": {
        "id": "c30e0479"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.activation = nn.GELU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "abf6b0f3",
      "metadata": {
        "id": "abf6b0f3"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.self_attn(x, x, x)\n",
        "        x = self.drop(x)\n",
        "        x = self.norm1(x + residual)\n",
        "\n",
        "        residual = x\n",
        "        x = self.feed_forward(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.norm2(x + residual)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "ef371208",
      "metadata": {
        "id": "ef371208"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_feature,num_layers, d_model, num_heads, d_ff, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = Embedding(num_feature,d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "ba857f80",
      "metadata": {
        "id": "ba857f80"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads,dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads,dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encode_ouput, mask):\n",
        "        residual = x\n",
        "        x = self.self_attn(x, x, x, mask)\n",
        "        x = self.drop(x)\n",
        "        x = self.norm1(x + residual)\n",
        "\n",
        "        residual = x\n",
        "        x = self.enc_dec_attn(x, encode_ouput, encode_ouput)\n",
        "        x = self.drop(x)\n",
        "        x = self.norm2(x + residual)\n",
        "\n",
        "        residual = x\n",
        "        x = self.feed_forward(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.norm3(x + residual)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "2ff51f3e",
      "metadata": {
        "id": "2ff51f3e"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_feature,num_layers, d_model, num_heads, d_ff, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = Embedding(num_feature,d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(d_model, num_feature)\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_output, mask)\n",
        "\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "9c2e5698",
      "metadata": {
        "id": "9c2e5698"
      },
      "outputs": [],
      "source": [
        "def create_look_ahead_mask(size, device=None):\n",
        "    mask = torch.triu(torch.ones(size, size), diagonal=1).type(torch.uint8)\n",
        "    mask = mask == 0\n",
        "    if device is not None:\n",
        "        mask = mask.to(device)\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validity_loss(x):\n",
        "    \"\"\"\n",
        "    计算预测结果的合法性损失\n",
        "    重点惩罚最低价高于最高价的情况\n",
        "    \"\"\"\n",
        "    # 提取价格列 (open, high, low, close)\n",
        "    # 假设预测结果的列顺序为: [open, high, low, close, volume, amount, turn, pctChg]\n",
        "    high = x[:, :, 1]  # 最高价\n",
        "    low = x[:, :, 2]   # 最低价\n",
        "    open_price = x[:, :, 0]  # 开盘价\n",
        "    close = x[:, :, 3]  # 收盘价\n",
        "\n",
        "    # 1. 最低价不能高于最高价\n",
        "    low_high_violation = torch.relu(low - high)  # 当low > high时，返回正差值\n",
        "\n",
        "    # 2. 开盘价和收盘价应该在最低价和最高价之间\n",
        "    open_bounds_violation = torch.relu(open_price - high) + torch.relu(low - open_price)\n",
        "    close_bounds_violation = torch.relu(close - high) + torch.relu(low - close)\n",
        "\n",
        "    # 3. 开盘价和收盘价不能为负\n",
        "    price_non_negative = torch.relu(-open_price) + torch.relu(-close) + torch.relu(-high) + torch.relu(-low)\n",
        "\n",
        "    # 组合所有损失，给最低价高于最高价的情况赋予更高的权重\n",
        "    total_violation = (\n",
        "        10.0 * low_high_violation +  # 最高权重给最低价高于最高价的情况\n",
        "        5.0 * open_bounds_violation +\n",
        "        5.0 * close_bounds_violation +\n",
        "        12.0 * price_non_negative\n",
        "    )\n",
        "\n",
        "    return total_violation.mean()"
      ],
      "metadata": {
        "id": "mI2baRvWmJpD"
      },
      "id": "mI2baRvWmJpD",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def volatility_loss(x):\n",
        "  batch_size, seq_len, num_features = x.shape\n",
        "  price_features = x[:, :, :4]\n",
        "  close = x[:, :, 3]  # 收盘价是第4个特征(索引3)\n",
        "  x_returns = (close[:, 1:] - close[:, :-1]) / close[:, :-1]\n",
        "  x_volatility = torch.std(x_returns, dim=1, keepdim=True)\n",
        "\n",
        "  return x_volatility\n"
      ],
      "metadata": {
        "id": "DwlPEf_GLIlA"
      },
      "id": "DwlPEf_GLIlA",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "57661e34",
      "metadata": {
        "id": "57661e34"
      },
      "outputs": [],
      "source": [
        "def infer(encoder, decoder, inputs, infer_steps):\n",
        "    \"\"\"\n",
        "    inputs: [1, seq_len, num_features]\n",
        "    results: [infer_steps, num_features]\n",
        "    \"\"\"\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    # 获取模型所在的设备\n",
        "    device = next(encoder.parameters()).device\n",
        "\n",
        "    # 确保输入数据在正确的设备上\n",
        "    inputs = inputs.to(device)\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        shift_outputs = inputs[:, -1:, :]\n",
        "\n",
        "        encode = encoder(inputs)\n",
        "        for _ in range(infer_steps):\n",
        "            mask_size = shift_outputs.size(1)\n",
        "            # 确保mask在正确的设备上\n",
        "            mask = create_look_ahead_mask(mask_size, device)\n",
        "\n",
        "            decode = decoder(shift_outputs, encode, mask)\n",
        "\n",
        "            results.append(decode[:, -1, :])\n",
        "\n",
        "            shift_outputs = torch.cat((shift_outputs, decode[:, -1, :].unsqueeze(1)), dim=1)\n",
        "\n",
        "    return torch.cat(results, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "92070bf5",
      "metadata": {
        "id": "92070bf5"
      },
      "outputs": [],
      "source": [
        "def prepare_data(data, lookback, forecast):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - lookback - forecast + 1):\n",
        "        X.append(data[i:i+lookback])\n",
        "        y.append(data[i+lookback:i+lookback+forecast])\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "ec5f0394",
      "metadata": {
        "id": "ec5f0394"
      },
      "outputs": [],
      "source": [
        "def fetch_data_loader():\n",
        "\n",
        "    bs.login()\n",
        "    rs = bs.query_history_k_data_plus(\"sh.600000\",\n",
        "        \"date,code,open,high,low,close,preclose,volume,amount,adjustflag,turn,tradestatus,pctChg,isST\",\n",
        "        start_date='2018-01-01', end_date='2024-12-31',\n",
        "        frequency=\"d\", adjustflag=\"3\")\n",
        "    data_list = []\n",
        "    while (rs.error_code == '0') & rs.next():\n",
        "        data_list.append(rs.get_row_data())\n",
        "    df = pd.DataFrame(data_list, columns=rs.fields)\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df = df.sort_values(by='date')\n",
        "    df = df.reset_index(drop=True)\n",
        "    df = df[['open', 'high', 'low', 'close', 'volume', 'amount', 'turn', 'pctChg']]\n",
        "    df = df.astype(float)\n",
        "\n",
        "    print(df.head())\n",
        "\n",
        "    bs.logout()\n",
        "\n",
        "    # 对数变换\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "    data = df.values\n",
        "\n",
        "    # 对价格和成交量应用对数变换\n",
        "    # 注意：pctChg已经是百分比变化，不需要变换\n",
        "    price_columns = [0, 1, 2, 3]  # open, high, low, close\n",
        "    volume_columns = [4, 5]  # volume, amount\n",
        "\n",
        "    # 应用对数变换（加上一个小常数避免log(0)）\n",
        "    data[:, price_columns] = np.log(data[:, price_columns] + 1e-8)\n",
        "    data[:, volume_columns] = np.log(data[:, volume_columns] + 1e-8)\n",
        "\n",
        "    X, y = prepare_data(data, LOOKBACK, FORECAST)\n",
        "\n",
        "    # 先划分训练集和验证集\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED, shuffle=False)\n",
        "\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "    y_val = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss(predicted, target):\n",
        "\n",
        "  pred_open = predicted[:, :, 0]\n",
        "  pred_close = predicted[:, :, 3]\n",
        "  tar_open = target[:, :, 0]\n",
        "  tar_close = target[:, :, 3]\n",
        "\n",
        "  # 计算预测序列的波动率\n",
        "\n",
        "  pred_returns = (pred_close[:, 1:] - pred_close[:, :-1]) / pred_close[:, :-1]\n",
        "  pred_volatility = torch.var(pred_returns, dim=1)\n",
        "\n",
        "  target_close = target[:, :, 3]\n",
        "  target_returns = (target_close[:, 1:] - target_close[:, :-1]) / target_close[:, :-1]\n",
        "  target_volatility = torch.var(target_returns, dim=1)\n",
        "\n",
        "  vol_loss = nn.functional.huber_loss(pred_volatility, target_volatility, reduction ='mean')\n",
        "\n",
        "  # 非法\n",
        "  vaild_loss = validity_loss(predicted)\n",
        "\n",
        "  # 涨跌比例\n",
        "  # 计算涨跌比例 - 鼓励50%的上涨天数\n",
        "  daily_changes = pred_close - pred_open  # 每日价格变化\n",
        "  up_days = (daily_changes > 0).float()   # 上涨的天数为1，下跌为0\n",
        "\n",
        "  # 计算上涨天数比例\n",
        "  pred_up_ratio = up_days.mean(dim=1)  # 每个序列的上涨比例\n",
        "\n",
        "  # 鼓励上涨比例接近50%\n",
        "  rate_loss = torch.abs(pred_up_ratio - torch.tensor(0.5, device=pred_up_ratio.device)).mean()\n",
        "\n",
        "  # 值\n",
        "  val_loss = nn.functional.huber_loss(pred_close[:, -1], tar_close[:, -1], reduction ='mean')\n",
        "\n",
        "  return val_coef * val_loss + vaild_coef * vaild_loss + vol_coef * vol_loss + rate_coef * rate_loss, val_loss, rate_loss"
      ],
      "metadata": {
        "id": "0uiBmqfel9_N"
      },
      "id": "0uiBmqfel9_N",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "f4f01bdc",
      "metadata": {
        "id": "f4f01bdc"
      },
      "outputs": [],
      "source": [
        "def train(encoder, decoder, train_loader, val_loader, lr, epochs, patience):\n",
        "    # 使用全局 DEVICE 变量\n",
        "    encoder.to(DEVICE)\n",
        "    decoder.to(DEVICE)\n",
        "\n",
        "    params = list(encoder.parameters()) + list(decoder.parameters())\n",
        "    optimizer = torch.optim.Adam(params, lr=lr)\n",
        "\n",
        "    # 早停机制相关变量\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = {\n",
        "        'encoder': None,\n",
        "        'decoder': None,\n",
        "        'epoch': -1,\n",
        "        'val_loss': float('inf')\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        encoder.train()\n",
        "        decoder.train()\n",
        "        train_loss = 0\n",
        "        train_vaild_loss = 0\n",
        "        train_rate_loss = 0\n",
        "\n",
        "        for train_x, train_y in train_loader:\n",
        "            train_x, train_y = train_x.to(DEVICE), train_y.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            enc_output = encoder(train_x)\n",
        "\n",
        "            mask_size = train_y.size(1)\n",
        "            # 使用全局 DEVICE 变量\n",
        "            mask = create_look_ahead_mask(mask_size, DEVICE)\n",
        "\n",
        "            decoder_input = torch.cat([train_x[:, -1:, :], train_y[:, :-1, :]], dim=1)\n",
        "\n",
        "            dec_output = decoder(decoder_input, enc_output, mask)\n",
        "\n",
        "            loss, vaild_loss, rate_loss = get_loss(dec_output, train_y)\n",
        "            loss.backward()\n",
        "\n",
        "            # 添加梯度裁剪，防止梯度爆炸\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_vaild_loss += vaild_loss.item()\n",
        "            train_rate_loss += rate_loss.item()\n",
        "\n",
        "        train_avg_loss = train_loss / len(train_loader)\n",
        "        train_avg_vaild_loss = train_vaild_loss / len(train_loader)\n",
        "        train_avg_rate_loss = train_rate_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Total Loss: {train_avg_loss:.4f}, Vaild Loss: {train_avg_vaild_loss:.4f}, Rate Loss: {train_avg_rate_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        encoder.eval()\n",
        "        decoder.eval()\n",
        "        val_loss = 0\n",
        "        val_vaild_loss = 0\n",
        "        val_rate_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for val_x, val_y in val_loader:\n",
        "                val_x, val_y = val_x.to(DEVICE), val_y.to(DEVICE)\n",
        "\n",
        "                enc_output = encoder(val_x)\n",
        "\n",
        "                mask_size = val_y.size(1)\n",
        "                # 使用全局 DEVICE 变量\n",
        "                mask = create_look_ahead_mask(mask_size, DEVICE)\n",
        "\n",
        "                decoder_input = torch.cat([val_x[:, -1:, :], val_y[:, :-1, :]], dim=1)\n",
        "\n",
        "                dec_output = decoder(decoder_input, enc_output, mask)\n",
        "\n",
        "                # 组合损失\n",
        "                loss, vaild_loss, rate_loss = get_loss(dec_output, val_y)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_vaild_loss += vaild_loss.item()\n",
        "                val_rate_loss += rate_loss.item()\n",
        "\n",
        "        val_avg_loss = val_loss / len(val_loader)\n",
        "        val_avg_vaild_loss = val_vaild_loss / len(val_loader)\n",
        "        val_avg_rate_loss = val_rate_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Val - Total Loss: {val_avg_loss:.4f}, Vaild Loss: {val_avg_vaild_loss:.4f}, Rate Loss: {val_avg_rate_loss:.4f}\")\n",
        "\n",
        "        # 早停机制检查\n",
        "        if val_avg_loss < best_val_loss:\n",
        "            best_val_loss = val_avg_loss\n",
        "            patience_counter = 0\n",
        "            # 保存最佳模型状态\n",
        "            best_model_state = {\n",
        "                'encoder': encoder.state_dict().copy(),\n",
        "                'decoder': decoder.state_dict().copy(),\n",
        "                'epoch': epoch,\n",
        "                'val_loss': val_avg_loss\n",
        "            }\n",
        "            print(f\"Validation loss improved to {val_avg_loss:.4f}, saving model...\")\n",
        "        else:\n",
        "            patience_counter += ((val_avg_vaild_loss < 1e-3) and (val_avg_rate_loss < 1e-2))\n",
        "            print(f\"Validation loss did not improve. Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "        # 检查是否应该早停\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs. Best validation loss: {best_val_loss:.4f}\")\n",
        "            # 恢复最佳模型\n",
        "            encoder.load_state_dict(best_model_state['encoder'])\n",
        "            decoder.load_state_dict(best_model_state['decoder'])\n",
        "            break\n",
        "\n",
        "    # 如果没有早停，确保最后保存的是最佳模型\n",
        "    if patience_counter < patience:\n",
        "        encoder.load_state_dict(best_model_state['encoder'])\n",
        "        decoder.load_state_dict(best_model_state['decoder'])\n",
        "        print(f\"Training completed. Best validation loss: {best_val_loss:.4f} at epoch {best_model_state['epoch']+1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ab64611",
      "metadata": {
        "id": "9ab64611",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "787acd33-a5f3-4fd7-e457-1da87c1e71a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "login success!\n",
            "    open   high    low  close      volume       amount      turn    pctChg\n",
            "0  12.61  12.77  12.60  12.72  31323053.0  398614966.0  0.111455  1.032566\n",
            "1  12.73  12.80  12.66  12.66  37839101.0  480954809.0  0.134641 -0.471701\n",
            "2  12.70  12.73  12.62  12.66  27883804.0  353205838.0  0.099217  0.000000\n",
            "3  12.67  12.71  12.62  12.69  31026744.0  393058250.0  0.110401  0.236965\n",
            "4  12.69  12.71  12.63  12.68  31389887.0  397842209.0  0.111693 -0.078797\n",
            "logout success!\n",
            "Epoch [1/20], Total Loss: 394.6772, Vaild Loss: 0.8019, Rate Loss: 0.4865\n",
            "Val - Total Loss: 250.1063, Vaild Loss: 1.0632, Rate Loss: 0.5000\n",
            "Validation loss improved to 250.1063, saving model...\n",
            "Epoch [2/20], Total Loss: 250.0093, Vaild Loss: 0.0492, Rate Loss: 0.5000\n",
            "Val - Total Loss: 250.0413, Vaild Loss: 0.4134, Rate Loss: 0.5000\n",
            "Validation loss improved to 250.0413, saving model...\n",
            "Epoch [3/20], Total Loss: 250.0032, Vaild Loss: 0.0294, Rate Loss: 0.5000\n",
            "Val - Total Loss: 250.0499, Vaild Loss: 0.4992, Rate Loss: 0.5000\n",
            "Validation loss did not improve. Patience: 0/5\n",
            "Epoch [4/20], Total Loss: 250.0029, Vaild Loss: 0.0255, Rate Loss: 0.5000\n",
            "Val - Total Loss: 250.0446, Vaild Loss: 0.4464, Rate Loss: 0.5000\n",
            "Validation loss did not improve. Patience: 0/5\n",
            "Epoch [5/20], Total Loss: 250.0257, Vaild Loss: 0.0431, Rate Loss: 0.5000\n",
            "Val - Total Loss: 250.0404, Vaild Loss: 0.4043, Rate Loss: 0.5000\n",
            "Validation loss improved to 250.0404, saving model...\n",
            "Epoch [6/20], Total Loss: 250.0027, Vaild Loss: 0.0263, Rate Loss: 0.5000\n",
            "Val - Total Loss: 250.0587, Vaild Loss: 0.5865, Rate Loss: 0.5000\n",
            "Validation loss did not improve. Patience: 0/5\n"
          ]
        }
      ],
      "source": [
        "# 在训练前添加设备检查\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# 获取数据\n",
        "train_loader, val_loader = fetch_data_loader()\n",
        "\n",
        "# 获取特征数量\n",
        "num_features = train_loader.dataset[0][0].shape[-1]\n",
        "\n",
        "# 初始化模型\n",
        "encoder = Encoder(num_features, NUM_LAYERS, D_MODEL, NUM_HEADS, D_FF, DROPOUT)\n",
        "decoder = Decoder(num_features, NUM_LAYERS, D_MODEL, NUM_HEADS, D_FF, DROPOUT)\n",
        "\n",
        "# 训练模型\n",
        "train(encoder, decoder, train_loader, val_loader, lr=LR, epochs=EPOCHS, patience=PATIENCE)\n",
        "\n",
        "# 示例推理\n",
        "# 获取一个样本进行推理\n",
        "sample_input, sample_output = val_loader.dataset[0]\n",
        "sample_input = sample_input.unsqueeze(0)  # 添加批次维度\n",
        "\n",
        "# 确保模型在正确的设备上\n",
        "encoder.to(DEVICE)\n",
        "decoder.to(DEVICE)\n",
        "sample_input = sample_input.to(DEVICE)\n",
        "\n",
        "predictions = infer(encoder, decoder, sample_input, FORECAST)\n",
        "\n",
        "predictions = predictions.cpu().numpy()\n",
        "sample_output = sample_output.numpy()\n",
        "\n",
        "# 反对数变换\n",
        "# 对价格和成交量应用对数变换\n",
        "# 注意：pctChg已经是百分比变化，不需要变换\n",
        "price_columns = [0, 1, 2, 3]  # open, high, low, close\n",
        "volume_columns = [4, 5]  # volume, amount\n",
        "\n",
        "# 应用对数变换（加上一个小常数避免log(0)）\n",
        "predictions[:, price_columns] = np.exp(predictions[:, price_columns]) - 1e-8\n",
        "predictions[:, volume_columns] = np.exp(predictions[:, volume_columns]) - 1e-8\n",
        "sample_output[:, price_columns] = np.exp(sample_output[:, price_columns]) - 1e-8\n",
        "sample_output[:, volume_columns] = np.exp(sample_output[:, volume_columns]) - 1e-8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "515ed9a6",
      "metadata": {
        "id": "515ed9a6"
      },
      "outputs": [],
      "source": [
        "# 'open', 'high', 'low', 'close', 'volume', 'amount', 'turn', 'pctChg'\n",
        "print(predictions.shape)  # 应该是 (FORECAST, num_features) (7, 8)\n",
        "print(\"Predictions:\", predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f8b9f3e",
      "metadata": {
        "id": "7f8b9f3e"
      },
      "outputs": [],
      "source": [
        "def plot_kline(predictions, start_date=None):\n",
        "    \"\"\"\n",
        "    绘制7天K线图\n",
        "    \"\"\"\n",
        "    # 验证输入\n",
        "    assert predictions.shape == (7, 8), f\"Expected (7,8), got {predictions.shape}\"\n",
        "\n",
        "    ohlc = predictions[:, :4]  # open, high, low, close\n",
        "    volume = predictions[:, 4]  # volume\n",
        "\n",
        "    # 处理日期\n",
        "    if start_date is None:\n",
        "        start_date = datetime.now()\n",
        "    else:\n",
        "        start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "\n",
        "    # 只保留日期，去除时间部分\n",
        "    start_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "    dates = [start_date + timedelta(days=i) for i in range(7)]\n",
        "\n",
        "    EPS = 1e-5  # 容差，防止浮点误差误报\n",
        "\n",
        "    for i in range(7):\n",
        "        o, h, l, c = ohlc[i]\n",
        "\n",
        "        if h < l - EPS:\n",
        "            print(f\"警告: high < low [H={h:.4f}, L={l:.4f}]\")\n",
        "        elif o < l - EPS or o > h + EPS:\n",
        "            print(f\"警告: open 超出 [low, high] 范围 [O={o:.4f}, L={l:.4f}, H={h:.4f}]\")\n",
        "        elif c < l - EPS or c > h + EPS:\n",
        "            print(f\"警告: close 超出 [low, high] 范围 [C={c:.4f}, L={l:.4f}, H={h:.4f}]\")\n",
        "\n",
        "    # 准备 OHLC 数据\n",
        "    ohlc_data = [\n",
        "        [date2num(date), o, h, l, c]\n",
        "        for date, (o, h, l, c) in zip(dates, ohlc)\n",
        "    ]\n",
        "\n",
        "    # 创建子图\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), gridspec_kw={'height_ratios': [3, 1]})\n",
        "\n",
        "    # 绘制 K 线\n",
        "    candlestick_ohlc(ax1, ohlc_data, width=0.6, colorup='red', colordown='green')  # A股习惯：红涨绿跌\n",
        "    ax1.set_title('7-Day Stock Price Prediction', fontsize=14)\n",
        "    ax1.set_ylabel('price')\n",
        "    ax1.xaxis.set_major_formatter(DateFormatter('%m-%d'))  # 显示月-日\n",
        "    ax1.xaxis.set_major_locator(DayLocator())\n",
        "    ax1.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "    # 旋转标签\n",
        "    for label in ax1.xaxis.get_majorticklabels():\n",
        "        label.set_rotation(45)\n",
        "\n",
        "    # 绘制成交量\n",
        "    ax2.bar([date2num(date) for date in dates], volume, width=0.6, color='gray', alpha=0.7)\n",
        "    ax2.set_ylabel('Volume')\n",
        "    ax2.xaxis.set_major_formatter(DateFormatter('%m-%d'))\n",
        "    ax2.xaxis.set_major_locator(DayLocator())\n",
        "    ax2.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "    for label in ax2.xaxis.get_majorticklabels():\n",
        "        label.set_rotation(45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e8b97a",
      "metadata": {
        "id": "62e8b97a"
      },
      "outputs": [],
      "source": [
        "plot_kline(sample_output, start_date=\"2024-01-01\")\n",
        "print(sample_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc5608e",
      "metadata": {
        "id": "bcc5608e"
      },
      "outputs": [],
      "source": [
        "plot_kline(predictions, start_date=\"2024-01-01\")\n",
        "print(predictions)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}