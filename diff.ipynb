{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90114b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "2.8.0+cu126\n",
      "12.6\n",
      "True\n",
      "\n",
      "==================================================\n",
      "Using device: cuda\n",
      "==================================================\n",
      "\n",
      "login success!\n",
      "原始数据前5行:\n",
      "    open   high    low  close      volume       amount      turn    pctChg\n",
      "0  12.61  12.77  12.60  12.72  31323053.0  398614966.0  0.111455  1.032566\n",
      "1  12.73  12.80  12.66  12.66  37839101.0  480954809.0  0.134641 -0.471701\n",
      "2  12.70  12.73  12.62  12.66  27883804.0  353205838.0  0.099217  0.000000\n",
      "3  12.67  12.71  12.62  12.69  31026744.0  393058250.0  0.110401  0.236965\n",
      "4  12.69  12.71  12.63  12.68  31389887.0  397842209.0  0.111693 -0.078797\n",
      "logout success!\n",
      "\n",
      "变化率数据前5行:\n",
      "[[ 1.26100000e+01  1.27700000e+01  1.26000000e+01  1.27200000e+01\n",
      "   3.13230530e+07  3.98614966e+08  1.11455000e-01  0.00000000e+00]\n",
      " [ 9.51625694e-01  2.34925607e-01  4.76190476e-01 -4.71698113e-01\n",
      "   2.08027232e+01  2.06564856e+01  1.34641000e-01 -4.71701000e-01]\n",
      " [-2.35663786e-01 -5.46875000e-01 -3.15955766e-01  0.00000000e+00\n",
      "  -2.63095495e+01 -2.65615331e+01  9.92170000e-02  0.00000000e+00]\n",
      " [-2.36220472e-01 -1.57109191e-01  0.00000000e+00  2.36966825e-01\n",
      "   1.12715611e+01  1.12830559e+01  1.10401000e-01  2.36965000e-01]\n",
      " [ 1.57853197e-01  0.00000000e+00  7.92393027e-02 -7.88022065e-02\n",
      "   1.17041930e+00  1.21711197e+00  1.11693000e-01 -7.87970000e-02]]\n",
      "Epoch [1/2000], Total Loss: 481.6655, Change Loss: 474.2595, Valid Loss: 1.3443, Vol Loss: 0.3421\n",
      "Val - Total Loss: 490.6491, Change Loss: 489.4835, Valid Loss: 0.0000, Vol Loss: 0.5828\n",
      "Validation loss improved to 490.6491, saving model...\n",
      "Epoch [2/2000], Total Loss: 462.1469, Change Loss: 461.3509, Valid Loss: 0.0341, Vol Loss: 0.3126\n",
      "Val - Total Loss: 483.2071, Change Loss: 482.8035, Valid Loss: 0.0000, Vol Loss: 0.2018\n",
      "Validation loss improved to 483.2071, saving model...\n",
      "Epoch [3/2000], Total Loss: 456.5855, Change Loss: 455.9611, Valid Loss: 0.0315, Vol Loss: 0.2335\n",
      "Val - Total Loss: 482.1417, Change Loss: 481.8127, Valid Loss: 0.0000, Vol Loss: 0.1645\n",
      "Validation loss improved to 482.1417, saving model...\n",
      "Epoch [4/2000], Total Loss: 455.0523, Change Loss: 454.5893, Valid Loss: 0.0155, Vol Loss: 0.1928\n",
      "Val - Total Loss: 477.6048, Change Loss: 477.3257, Valid Loss: 0.0000, Vol Loss: 0.1395\n",
      "Validation loss improved to 477.6048, saving model...\n",
      "Epoch [5/2000], Total Loss: 452.7991, Change Loss: 452.3058, Valid Loss: 0.0206, Vol Loss: 0.1950\n",
      "Val - Total Loss: 478.3888, Change Loss: 478.0706, Valid Loss: 0.0000, Vol Loss: 0.1591\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [6/2000], Total Loss: 452.8397, Change Loss: 452.3180, Valid Loss: 0.0220, Vol Loss: 0.2059\n",
      "Val - Total Loss: 477.3672, Change Loss: 477.0673, Valid Loss: 0.0000, Vol Loss: 0.1500\n",
      "Validation loss improved to 477.3672, saving model...\n",
      "Epoch [7/2000], Total Loss: 450.7679, Change Loss: 450.2704, Valid Loss: 0.0214, Vol Loss: 0.1953\n",
      "Val - Total Loss: 476.8137, Change Loss: 476.4840, Valid Loss: 0.0000, Vol Loss: 0.1649\n",
      "Validation loss improved to 476.8137, saving model...\n",
      "Epoch [8/2000], Total Loss: 449.1500, Change Loss: 448.6654, Valid Loss: 0.0165, Vol Loss: 0.2010\n",
      "Val - Total Loss: 475.5124, Change Loss: 475.1922, Valid Loss: 0.0000, Vol Loss: 0.1601\n",
      "Validation loss improved to 475.5124, saving model...\n",
      "Epoch [9/2000], Total Loss: 448.9318, Change Loss: 448.4276, Valid Loss: 0.0217, Vol Loss: 0.1979\n",
      "Val - Total Loss: 475.7430, Change Loss: 475.4221, Valid Loss: 0.0000, Vol Loss: 0.1604\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [10/2000], Total Loss: 448.3567, Change Loss: 447.9163, Valid Loss: 0.0124, Vol Loss: 0.1892\n",
      "Val - Total Loss: 472.4825, Change Loss: 472.1490, Valid Loss: 0.0000, Vol Loss: 0.1668\n",
      "Validation loss improved to 472.4825, saving model...\n",
      "Epoch [11/2000], Total Loss: 446.6056, Change Loss: 446.1157, Valid Loss: 0.0190, Vol Loss: 0.1975\n",
      "Val - Total Loss: 466.2355, Change Loss: 465.9275, Valid Loss: 0.0000, Vol Loss: 0.1540\n",
      "Validation loss improved to 466.2355, saving model...\n",
      "Epoch [12/2000], Total Loss: 446.9660, Change Loss: 446.5163, Valid Loss: 0.0141, Vol Loss: 0.1895\n",
      "Val - Total Loss: 466.7402, Change Loss: 466.4229, Valid Loss: 0.0000, Vol Loss: 0.1586\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [13/2000], Total Loss: 446.2268, Change Loss: 445.8076, Valid Loss: 0.0100, Vol Loss: 0.1846\n",
      "Val - Total Loss: 459.0422, Change Loss: 458.7456, Valid Loss: 0.0000, Vol Loss: 0.1483\n",
      "Validation loss improved to 459.0422, saving model...\n",
      "Epoch [14/2000], Total Loss: 445.0656, Change Loss: 444.6251, Valid Loss: 0.0123, Vol Loss: 0.1895\n",
      "Val - Total Loss: 456.9509, Change Loss: 456.6294, Valid Loss: 0.0000, Vol Loss: 0.1608\n",
      "Validation loss improved to 456.9509, saving model...\n",
      "Epoch [15/2000], Total Loss: 445.5978, Change Loss: 445.1620, Valid Loss: 0.0130, Vol Loss: 0.1855\n",
      "Val - Total Loss: 462.0369, Change Loss: 461.7465, Valid Loss: 0.0000, Vol Loss: 0.1452\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [16/2000], Total Loss: 444.6892, Change Loss: 444.2451, Valid Loss: 0.0115, Vol Loss: 0.1934\n",
      "Val - Total Loss: 462.4467, Change Loss: 462.1041, Valid Loss: 0.0000, Vol Loss: 0.1713\n",
      "Validation loss did not improve. Patience: 2/100\n",
      "Epoch [17/2000], Total Loss: 445.8055, Change Loss: 445.3637, Valid Loss: 0.0129, Vol Loss: 0.1886\n",
      "Val - Total Loss: 468.8213, Change Loss: 468.4891, Valid Loss: 0.0000, Vol Loss: 0.1661\n",
      "Validation loss did not improve. Patience: 3/100\n",
      "Epoch [18/2000], Total Loss: 444.8564, Change Loss: 444.4317, Valid Loss: 0.0103, Vol Loss: 0.1865\n",
      "Val - Total Loss: 459.9099, Change Loss: 459.5980, Valid Loss: 0.0000, Vol Loss: 0.1559\n",
      "Validation loss did not improve. Patience: 4/100\n",
      "Epoch [19/2000], Total Loss: 443.8838, Change Loss: 443.4609, Valid Loss: 0.0100, Vol Loss: 0.1864\n",
      "Val - Total Loss: 461.4130, Change Loss: 461.1248, Valid Loss: 0.0000, Vol Loss: 0.1441\n",
      "Validation loss did not improve. Patience: 5/100\n",
      "Epoch [20/2000], Total Loss: 443.5051, Change Loss: 443.0552, Valid Loss: 0.0134, Vol Loss: 0.1914\n",
      "Val - Total Loss: 467.3973, Change Loss: 467.0614, Valid Loss: 0.0000, Vol Loss: 0.1679\n",
      "Validation loss did not improve. Patience: 6/100\n",
      "Epoch [21/2000], Total Loss: 441.0974, Change Loss: 440.6643, Valid Loss: 0.0118, Vol Loss: 0.1869\n",
      "Val - Total Loss: 466.0632, Change Loss: 465.7621, Valid Loss: 0.0000, Vol Loss: 0.1506\n",
      "Validation loss did not improve. Patience: 7/100\n",
      "Epoch [22/2000], Total Loss: 442.7310, Change Loss: 442.2763, Valid Loss: 0.0123, Vol Loss: 0.1966\n",
      "Val - Total Loss: 468.8803, Change Loss: 468.5333, Valid Loss: 0.0000, Vol Loss: 0.1735\n",
      "Validation loss did not improve. Patience: 8/100\n",
      "Epoch [23/2000], Total Loss: 443.1054, Change Loss: 442.6864, Valid Loss: 0.0089, Vol Loss: 0.1873\n",
      "Val - Total Loss: 464.5488, Change Loss: 464.2613, Valid Loss: 0.0000, Vol Loss: 0.1437\n",
      "Validation loss did not improve. Patience: 9/100\n",
      "Epoch [24/2000], Total Loss: 442.5954, Change Loss: 442.1687, Valid Loss: 0.0100, Vol Loss: 0.1884\n",
      "Val - Total Loss: 466.0394, Change Loss: 465.7404, Valid Loss: 0.0000, Vol Loss: 0.1495\n",
      "Validation loss did not improve. Patience: 10/100\n",
      "Epoch [25/2000], Total Loss: 442.1488, Change Loss: 441.7286, Valid Loss: 0.0094, Vol Loss: 0.1867\n",
      "Val - Total Loss: 465.2235, Change Loss: 464.8944, Valid Loss: 0.0000, Vol Loss: 0.1645\n",
      "Validation loss did not improve. Patience: 11/100\n",
      "Epoch [26/2000], Total Loss: 441.5220, Change Loss: 441.0808, Valid Loss: 0.0133, Vol Loss: 0.1872\n",
      "Val - Total Loss: 456.3300, Change Loss: 456.0288, Valid Loss: 0.0000, Vol Loss: 0.1506\n",
      "Validation loss improved to 456.3300, saving model...\n",
      "Epoch [27/2000], Total Loss: 440.0746, Change Loss: 439.6203, Valid Loss: 0.0137, Vol Loss: 0.1929\n",
      "Val - Total Loss: 457.0082, Change Loss: 456.7068, Valid Loss: 0.0000, Vol Loss: 0.1507\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [28/2000], Total Loss: 441.3453, Change Loss: 440.9116, Valid Loss: 0.0110, Vol Loss: 0.1893\n",
      "Val - Total Loss: 461.7417, Change Loss: 461.4549, Valid Loss: 0.0000, Vol Loss: 0.1434\n",
      "Validation loss did not improve. Patience: 2/100\n",
      "Epoch [29/2000], Total Loss: 440.4023, Change Loss: 439.9895, Valid Loss: 0.0094, Vol Loss: 0.1828\n",
      "Val - Total Loss: 463.5247, Change Loss: 463.2326, Valid Loss: 0.0000, Vol Loss: 0.1460\n",
      "Validation loss did not improve. Patience: 3/100\n",
      "Epoch [30/2000], Total Loss: 441.2428, Change Loss: 440.8133, Valid Loss: 0.0112, Vol Loss: 0.1868\n",
      "Val - Total Loss: 464.2703, Change Loss: 463.9595, Valid Loss: 0.0000, Vol Loss: 0.1554\n",
      "Validation loss did not improve. Patience: 4/100\n",
      "Epoch [31/2000], Total Loss: 440.3881, Change Loss: 439.9385, Valid Loss: 0.0106, Vol Loss: 0.1983\n",
      "Val - Total Loss: 465.8753, Change Loss: 465.5924, Valid Loss: 0.0000, Vol Loss: 0.1415\n",
      "Validation loss did not improve. Patience: 5/100\n",
      "Epoch [32/2000], Total Loss: 439.8114, Change Loss: 439.3940, Valid Loss: 0.0104, Vol Loss: 0.1828\n",
      "Val - Total Loss: 461.3867, Change Loss: 461.1121, Valid Loss: 0.0000, Vol Loss: 0.1373\n",
      "Validation loss did not improve. Patience: 6/100\n",
      "Epoch [33/2000], Total Loss: 440.7581, Change Loss: 440.3340, Valid Loss: 0.0094, Vol Loss: 0.1884\n",
      "Val - Total Loss: 461.0503, Change Loss: 460.7300, Valid Loss: 0.0000, Vol Loss: 0.1601\n",
      "Validation loss did not improve. Patience: 7/100\n",
      "Epoch [34/2000], Total Loss: 442.0551, Change Loss: 441.6473, Valid Loss: 0.0088, Vol Loss: 0.1820\n",
      "Val - Total Loss: 463.3038, Change Loss: 463.0105, Valid Loss: 0.0000, Vol Loss: 0.1466\n",
      "Validation loss did not improve. Patience: 8/100\n",
      "Epoch [35/2000], Total Loss: 440.5009, Change Loss: 440.0842, Valid Loss: 0.0088, Vol Loss: 0.1863\n",
      "Val - Total Loss: 461.1916, Change Loss: 460.9126, Valid Loss: 0.0000, Vol Loss: 0.1395\n",
      "Validation loss did not improve. Patience: 9/100\n",
      "Epoch [36/2000], Total Loss: 439.2875, Change Loss: 438.8662, Valid Loss: 0.0115, Vol Loss: 0.1818\n",
      "Val - Total Loss: 458.9515, Change Loss: 458.6821, Valid Loss: 0.0000, Vol Loss: 0.1347\n",
      "Validation loss did not improve. Patience: 10/100\n",
      "Epoch [37/2000], Total Loss: 439.1945, Change Loss: 438.7740, Valid Loss: 0.0106, Vol Loss: 0.1839\n",
      "Val - Total Loss: 459.4290, Change Loss: 459.1449, Valid Loss: 0.0000, Vol Loss: 0.1421\n",
      "Validation loss did not improve. Patience: 11/100\n",
      "Epoch [38/2000], Total Loss: 439.0898, Change Loss: 438.6718, Valid Loss: 0.0083, Vol Loss: 0.1883\n",
      "Val - Total Loss: 461.4009, Change Loss: 461.1193, Valid Loss: 0.0000, Vol Loss: 0.1408\n",
      "Validation loss did not improve. Patience: 12/100\n",
      "Epoch [39/2000], Total Loss: 438.8143, Change Loss: 438.4212, Valid Loss: 0.0077, Vol Loss: 0.1773\n",
      "Val - Total Loss: 462.5796, Change Loss: 462.2835, Valid Loss: 0.0000, Vol Loss: 0.1481\n",
      "Validation loss did not improve. Patience: 13/100\n",
      "Epoch [40/2000], Total Loss: 439.1706, Change Loss: 438.7492, Valid Loss: 0.0084, Vol Loss: 0.1898\n",
      "Val - Total Loss: 456.2469, Change Loss: 455.9687, Valid Loss: 0.0000, Vol Loss: 0.1391\n",
      "Validation loss improved to 456.2469, saving model...\n",
      "Epoch [41/2000], Total Loss: 438.5023, Change Loss: 438.0885, Valid Loss: 0.0098, Vol Loss: 0.1825\n",
      "Val - Total Loss: 457.1785, Change Loss: 456.8732, Valid Loss: 0.0000, Vol Loss: 0.1527\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [42/2000], Total Loss: 438.8224, Change Loss: 438.4206, Valid Loss: 0.0069, Vol Loss: 0.1836\n",
      "Val - Total Loss: 455.2862, Change Loss: 455.0109, Valid Loss: 0.0000, Vol Loss: 0.1376\n",
      "Validation loss improved to 455.2862, saving model...\n",
      "Epoch [43/2000], Total Loss: 437.5308, Change Loss: 437.1116, Valid Loss: 0.0090, Vol Loss: 0.1870\n",
      "Val - Total Loss: 457.6229, Change Loss: 457.3429, Valid Loss: 0.0000, Vol Loss: 0.1400\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [44/2000], Total Loss: 438.7763, Change Loss: 438.3772, Valid Loss: 0.0072, Vol Loss: 0.1815\n",
      "Val - Total Loss: 458.6597, Change Loss: 458.3820, Valid Loss: 0.0000, Vol Loss: 0.1389\n",
      "Validation loss did not improve. Patience: 2/100\n",
      "Epoch [46/2000], Total Loss: 438.9409, Change Loss: 438.5193, Valid Loss: 0.0074, Vol Loss: 0.1923\n",
      "Val - Total Loss: 456.8449, Change Loss: 456.5576, Valid Loss: 0.0000, Vol Loss: 0.1436\n",
      "Validation loss did not improve. Patience: 4/100\n",
      "Epoch [47/2000], Total Loss: 436.8260, Change Loss: 436.4056, Valid Loss: 0.0078, Vol Loss: 0.1906\n",
      "Val - Total Loss: 456.1235, Change Loss: 455.8339, Valid Loss: 0.0000, Vol Loss: 0.1448\n",
      "Validation loss did not improve. Patience: 5/100\n",
      "Epoch [48/2000], Total Loss: 437.4856, Change Loss: 437.0697, Valid Loss: 0.0079, Vol Loss: 0.1883\n",
      "Val - Total Loss: 456.3201, Change Loss: 456.0296, Valid Loss: 0.0000, Vol Loss: 0.1453\n",
      "Validation loss did not improve. Patience: 6/100\n",
      "Epoch [49/2000], Total Loss: 437.5485, Change Loss: 437.1491, Valid Loss: 0.0076, Vol Loss: 0.1806\n",
      "Val - Total Loss: 456.1942, Change Loss: 455.9227, Valid Loss: 0.0000, Vol Loss: 0.1358\n",
      "Validation loss did not improve. Patience: 7/100\n",
      "Epoch [50/2000], Total Loss: 438.0112, Change Loss: 437.6084, Valid Loss: 0.0060, Vol Loss: 0.1865\n",
      "Val - Total Loss: 454.6747, Change Loss: 454.3982, Valid Loss: 0.0000, Vol Loss: 0.1383\n",
      "Validation loss improved to 454.6747, saving model...\n",
      "Epoch [52/2000], Total Loss: 436.5855, Change Loss: 436.2000, Valid Loss: 0.0066, Vol Loss: 0.1764\n",
      "Val - Total Loss: 460.8213, Change Loss: 460.5529, Valid Loss: 0.0000, Vol Loss: 0.1342\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [53/2000], Total Loss: 437.3956, Change Loss: 436.9950, Valid Loss: 0.0068, Vol Loss: 0.1834\n",
      "Val - Total Loss: 457.4355, Change Loss: 457.1499, Valid Loss: 0.0000, Vol Loss: 0.1428\n",
      "Validation loss did not improve. Patience: 2/100\n",
      "Epoch [54/2000], Total Loss: 436.1363, Change Loss: 435.7147, Valid Loss: 0.0092, Vol Loss: 0.1877\n",
      "Val - Total Loss: 453.2868, Change Loss: 453.0028, Valid Loss: 0.0000, Vol Loss: 0.1420\n",
      "Validation loss improved to 453.2868, saving model...\n",
      "Epoch [55/2000], Total Loss: 437.8590, Change Loss: 437.4347, Valid Loss: 0.0084, Vol Loss: 0.1912\n",
      "Val - Total Loss: 456.4369, Change Loss: 456.1596, Valid Loss: 0.0000, Vol Loss: 0.1387\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [56/2000], Total Loss: 436.2611, Change Loss: 435.8689, Valid Loss: 0.0067, Vol Loss: 0.1794\n",
      "Val - Total Loss: 454.9882, Change Loss: 454.7199, Valid Loss: 0.0000, Vol Loss: 0.1341\n",
      "Validation loss did not improve. Patience: 2/100\n",
      "Epoch [57/2000], Total Loss: 437.1164, Change Loss: 436.7144, Valid Loss: 0.0078, Vol Loss: 0.1815\n",
      "Val - Total Loss: 459.5448, Change Loss: 459.2788, Valid Loss: 0.0000, Vol Loss: 0.1330\n",
      "Validation loss did not improve. Patience: 3/100\n",
      "Epoch [59/2000], Total Loss: 437.2575, Change Loss: 436.8710, Valid Loss: 0.0061, Vol Loss: 0.1779\n",
      "Val - Total Loss: 453.9402, Change Loss: 453.6648, Valid Loss: 0.0000, Vol Loss: 0.1377\n",
      "Validation loss did not improve. Patience: 5/100\n",
      "Epoch [60/2000], Total Loss: 436.5083, Change Loss: 436.1132, Valid Loss: 0.0064, Vol Loss: 0.1816\n",
      "Val - Total Loss: 457.2608, Change Loss: 456.9733, Valid Loss: 0.0000, Vol Loss: 0.1438\n",
      "Validation loss did not improve. Patience: 6/100\n",
      "Epoch [61/2000], Total Loss: 435.3613, Change Loss: 434.9348, Valid Loss: 0.0069, Vol Loss: 0.1961\n",
      "Val - Total Loss: 454.2587, Change Loss: 453.9843, Valid Loss: 0.0000, Vol Loss: 0.1372\n",
      "Validation loss did not improve. Patience: 7/100\n",
      "Epoch [62/2000], Total Loss: 436.1485, Change Loss: 435.7407, Valid Loss: 0.0071, Vol Loss: 0.1862\n",
      "Val - Total Loss: 451.2711, Change Loss: 450.9978, Valid Loss: 0.0000, Vol Loss: 0.1366\n",
      "Validation loss improved to 451.2711, saving model...\n",
      "Epoch [63/2000], Total Loss: 437.4539, Change Loss: 437.0630, Valid Loss: 0.0069, Vol Loss: 0.1781\n",
      "Val - Total Loss: 456.2415, Change Loss: 455.9795, Valid Loss: 0.0000, Vol Loss: 0.1310\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [64/2000], Total Loss: 434.8302, Change Loss: 434.4333, Valid Loss: 0.0057, Vol Loss: 0.1841\n",
      "Val - Total Loss: 457.1715, Change Loss: 456.9002, Valid Loss: 0.0000, Vol Loss: 0.1356\n",
      "Validation loss did not improve. Patience: 2/100\n",
      "Epoch [65/2000], Total Loss: 436.0796, Change Loss: 435.6694, Valid Loss: 0.0066, Vol Loss: 0.1886\n",
      "Val - Total Loss: 452.1895, Change Loss: 451.9258, Valid Loss: 0.0000, Vol Loss: 0.1319\n",
      "Validation loss did not improve. Patience: 3/100\n",
      "Epoch [66/2000], Total Loss: 434.7178, Change Loss: 434.3210, Valid Loss: 0.0068, Vol Loss: 0.1814\n",
      "Val - Total Loss: 453.2761, Change Loss: 453.0157, Valid Loss: 0.0000, Vol Loss: 0.1302\n",
      "Validation loss did not improve. Patience: 4/100\n",
      "Epoch [67/2000], Total Loss: 435.8805, Change Loss: 435.4901, Valid Loss: 0.0066, Vol Loss: 0.1786\n",
      "Val - Total Loss: 450.8265, Change Loss: 450.5487, Valid Loss: 0.0000, Vol Loss: 0.1389\n",
      "Validation loss improved to 450.8265, saving model...\n",
      "Epoch [68/2000], Total Loss: 435.0484, Change Loss: 434.6488, Valid Loss: 0.0058, Vol Loss: 0.1854\n",
      "Val - Total Loss: 451.5995, Change Loss: 451.3352, Valid Loss: 0.0000, Vol Loss: 0.1322\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [69/2000], Total Loss: 435.2533, Change Loss: 434.8506, Valid Loss: 0.0066, Vol Loss: 0.1850\n",
      "Val - Total Loss: 452.0763, Change Loss: 451.8176, Valid Loss: 0.0000, Vol Loss: 0.1294\n",
      "Validation loss did not improve. Patience: 2/100\n",
      "Epoch [70/2000], Total Loss: 433.9597, Change Loss: 433.5740, Valid Loss: 0.0049, Vol Loss: 0.1807\n",
      "Val - Total Loss: 453.3263, Change Loss: 453.0665, Valid Loss: 0.0000, Vol Loss: 0.1299\n",
      "Validation loss did not improve. Patience: 3/100\n",
      "Epoch [72/2000], Total Loss: 435.0604, Change Loss: 434.6566, Valid Loss: 0.0069, Vol Loss: 0.1846\n",
      "Val - Total Loss: 449.5957, Change Loss: 449.3199, Valid Loss: 0.0000, Vol Loss: 0.1379\n",
      "Validation loss improved to 449.5957, saving model...\n",
      "Epoch [73/2000], Total Loss: 432.7566, Change Loss: 432.3540, Valid Loss: 0.0051, Vol Loss: 0.1884\n",
      "Val - Total Loss: 451.0857, Change Loss: 450.8184, Valid Loss: 0.0000, Vol Loss: 0.1337\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [74/2000], Total Loss: 433.5562, Change Loss: 433.1370, Valid Loss: 0.0083, Vol Loss: 0.1888\n",
      "Val - Total Loss: 448.0189, Change Loss: 447.7512, Valid Loss: 0.0000, Vol Loss: 0.1339\n",
      "Validation loss improved to 448.0189, saving model...\n",
      "Epoch [75/2000], Total Loss: 435.8658, Change Loss: 435.4655, Valid Loss: 0.0050, Vol Loss: 0.1877\n",
      "Val - Total Loss: 449.5066, Change Loss: 449.2468, Valid Loss: 0.0000, Vol Loss: 0.1299\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [76/2000], Total Loss: 434.5042, Change Loss: 434.1177, Valid Loss: 0.0052, Vol Loss: 0.1803\n",
      "Val - Total Loss: 446.5346, Change Loss: 446.2775, Valid Loss: 0.0000, Vol Loss: 0.1286\n",
      "Validation loss improved to 446.5346, saving model...\n",
      "Epoch [77/2000], Total Loss: 432.9812, Change Loss: 432.5796, Valid Loss: 0.0059, Vol Loss: 0.1860\n",
      "Val - Total Loss: 444.6965, Change Loss: 444.4206, Valid Loss: 0.0000, Vol Loss: 0.1379\n",
      "Validation loss improved to 444.6965, saving model...\n",
      "Epoch [78/2000], Total Loss: 433.1272, Change Loss: 432.7307, Valid Loss: 0.0053, Vol Loss: 0.1849\n",
      "Val - Total Loss: 448.8238, Change Loss: 448.5233, Valid Loss: 0.0000, Vol Loss: 0.1502\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [79/2000], Total Loss: 431.9955, Change Loss: 431.5660, Valid Loss: 0.0071, Vol Loss: 0.1971\n",
      "Val - Total Loss: 449.0517, Change Loss: 448.7896, Valid Loss: 0.0000, Vol Loss: 0.1310\n",
      "Validation loss did not improve. Patience: 2/100\n",
      "Epoch [80/2000], Total Loss: 434.4727, Change Loss: 434.0838, Valid Loss: 0.0049, Vol Loss: 0.1822\n",
      "Val - Total Loss: 445.2960, Change Loss: 445.0353, Valid Loss: 0.0000, Vol Loss: 0.1304\n",
      "Validation loss did not improve. Patience: 3/100\n",
      "Epoch [81/2000], Total Loss: 432.8851, Change Loss: 432.4917, Valid Loss: 0.0056, Vol Loss: 0.1827\n",
      "Val - Total Loss: 444.7905, Change Loss: 444.5369, Valid Loss: 0.0000, Vol Loss: 0.1268\n",
      "Validation loss did not improve. Patience: 4/100\n",
      "Epoch [82/2000], Total Loss: 434.3153, Change Loss: 433.9262, Valid Loss: 0.0055, Vol Loss: 0.1808\n",
      "Val - Total Loss: 453.1025, Change Loss: 452.8448, Valid Loss: 0.0000, Vol Loss: 0.1288\n",
      "Validation loss did not improve. Patience: 5/100\n",
      "Epoch [83/2000], Total Loss: 433.6906, Change Loss: 433.3031, Valid Loss: 0.0044, Vol Loss: 0.1828\n",
      "Val - Total Loss: 448.5895, Change Loss: 448.3330, Valid Loss: 0.0000, Vol Loss: 0.1283\n",
      "Validation loss did not improve. Patience: 6/100\n",
      "Epoch [84/2000], Total Loss: 431.8182, Change Loss: 431.4187, Valid Loss: 0.0063, Vol Loss: 0.1839\n",
      "Val - Total Loss: 447.6558, Change Loss: 447.4061, Valid Loss: 0.0000, Vol Loss: 0.1249\n",
      "Validation loss did not improve. Patience: 7/100\n",
      "Epoch [85/2000], Total Loss: 432.5490, Change Loss: 432.1661, Valid Loss: 0.0046, Vol Loss: 0.1800\n",
      "Val - Total Loss: 445.9524, Change Loss: 445.6867, Valid Loss: 0.0000, Vol Loss: 0.1328\n",
      "Validation loss did not improve. Patience: 8/100\n",
      "Epoch [86/2000], Total Loss: 432.4957, Change Loss: 432.0906, Valid Loss: 0.0061, Vol Loss: 0.1874\n",
      "Val - Total Loss: 446.1060, Change Loss: 445.8501, Valid Loss: 0.0000, Vol Loss: 0.1280\n",
      "Validation loss did not improve. Patience: 9/100\n",
      "Epoch [87/2000], Total Loss: 431.2491, Change Loss: 430.8618, Valid Loss: 0.0051, Vol Loss: 0.1809\n",
      "Val - Total Loss: 445.4050, Change Loss: 445.1463, Valid Loss: 0.0000, Vol Loss: 0.1294\n",
      "Validation loss did not improve. Patience: 10/100\n",
      "Epoch [88/2000], Total Loss: 431.1822, Change Loss: 430.7660, Valid Loss: 0.0064, Vol Loss: 0.1920\n",
      "Val - Total Loss: 447.7100, Change Loss: 447.4538, Valid Loss: 0.0000, Vol Loss: 0.1281\n",
      "Validation loss did not improve. Patience: 11/100\n",
      "Epoch [89/2000], Total Loss: 433.6245, Change Loss: 433.2272, Valid Loss: 0.0058, Vol Loss: 0.1842\n",
      "Val - Total Loss: 445.2768, Change Loss: 445.0173, Valid Loss: 0.0000, Vol Loss: 0.1298\n",
      "Validation loss did not improve. Patience: 12/100\n",
      "Epoch [90/2000], Total Loss: 433.6801, Change Loss: 433.2969, Valid Loss: 0.0050, Vol Loss: 0.1791\n",
      "Val - Total Loss: 448.3451, Change Loss: 448.0605, Valid Loss: 0.0000, Vol Loss: 0.1423\n",
      "Validation loss did not improve. Patience: 13/100\n",
      "Epoch [92/2000], Total Loss: 432.7588, Change Loss: 432.3579, Valid Loss: 0.0059, Vol Loss: 0.1857\n",
      "Val - Total Loss: 449.1644, Change Loss: 448.9031, Valid Loss: 0.0000, Vol Loss: 0.1306\n",
      "Validation loss did not improve. Patience: 15/100\n",
      "Epoch [94/2000], Total Loss: 433.0554, Change Loss: 432.6784, Valid Loss: 0.0048, Vol Loss: 0.1765\n",
      "Val - Total Loss: 441.2215, Change Loss: 440.9579, Valid Loss: 0.0000, Vol Loss: 0.1318\n",
      "Validation loss improved to 441.2215, saving model...\n",
      "Epoch [95/2000], Total Loss: 431.1531, Change Loss: 430.7513, Valid Loss: 0.0046, Vol Loss: 0.1893\n",
      "Val - Total Loss: 443.1789, Change Loss: 442.9094, Valid Loss: 0.0000, Vol Loss: 0.1347\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [96/2000], Total Loss: 430.0241, Change Loss: 429.6285, Valid Loss: 0.0058, Vol Loss: 0.1834\n",
      "Val - Total Loss: 443.2266, Change Loss: 442.9668, Valid Loss: 0.0000, Vol Loss: 0.1299\n",
      "Validation loss did not improve. Patience: 2/100\n",
      "Epoch [97/2000], Total Loss: 431.6764, Change Loss: 431.2875, Valid Loss: 0.0042, Vol Loss: 0.1841\n",
      "Val - Total Loss: 441.6517, Change Loss: 441.3793, Valid Loss: 0.0000, Vol Loss: 0.1362\n",
      "Validation loss did not improve. Patience: 3/100\n",
      "Epoch [98/2000], Total Loss: 428.7202, Change Loss: 428.3219, Valid Loss: 0.0044, Vol Loss: 0.1882\n",
      "Val - Total Loss: 443.2879, Change Loss: 443.0164, Valid Loss: 0.0000, Vol Loss: 0.1358\n",
      "Validation loss did not improve. Patience: 4/100\n",
      "Epoch [99/2000], Total Loss: 429.2647, Change Loss: 428.8605, Valid Loss: 0.0053, Vol Loss: 0.1889\n",
      "Val - Total Loss: 446.0835, Change Loss: 445.8228, Valid Loss: 0.0000, Vol Loss: 0.1303\n",
      "Validation loss did not improve. Patience: 5/100\n",
      "Epoch [100/2000], Total Loss: 431.9046, Change Loss: 431.5310, Valid Loss: 0.0034, Vol Loss: 0.1783\n",
      "Val - Total Loss: 441.2798, Change Loss: 441.0247, Valid Loss: 0.0000, Vol Loss: 0.1275\n",
      "Validation loss did not improve. Patience: 6/100\n",
      "Epoch [101/2000], Total Loss: 429.9069, Change Loss: 429.5310, Valid Loss: 0.0037, Vol Loss: 0.1788\n",
      "Val - Total Loss: 439.8209, Change Loss: 439.5612, Valid Loss: 0.0000, Vol Loss: 0.1299\n",
      "Validation loss improved to 439.8209, saving model...\n",
      "Epoch [102/2000], Total Loss: 430.8481, Change Loss: 430.4589, Valid Loss: 0.0042, Vol Loss: 0.1840\n",
      "Val - Total Loss: 438.6498, Change Loss: 438.3804, Valid Loss: 0.0000, Vol Loss: 0.1347\n",
      "Validation loss improved to 438.6498, saving model...\n",
      "Epoch [103/2000], Total Loss: 430.5492, Change Loss: 430.1464, Valid Loss: 0.0047, Vol Loss: 0.1897\n",
      "Val - Total Loss: 444.3879, Change Loss: 444.1195, Valid Loss: 0.0000, Vol Loss: 0.1342\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [104/2000], Total Loss: 428.8843, Change Loss: 428.4904, Valid Loss: 0.0043, Vol Loss: 0.1863\n",
      "Val - Total Loss: 441.1560, Change Loss: 440.8963, Valid Loss: 0.0000, Vol Loss: 0.1299\n",
      "Validation loss did not improve. Patience: 2/100\n",
      "Epoch [105/2000], Total Loss: 429.2089, Change Loss: 428.8291, Valid Loss: 0.0045, Vol Loss: 0.1786\n",
      "Val - Total Loss: 440.0030, Change Loss: 439.7325, Valid Loss: 0.0000, Vol Loss: 0.1352\n",
      "Validation loss did not improve. Patience: 3/100\n",
      "Epoch [106/2000], Total Loss: 427.7457, Change Loss: 427.3386, Valid Loss: 0.0048, Vol Loss: 0.1915\n",
      "Val - Total Loss: 439.1970, Change Loss: 438.9126, Valid Loss: 0.0000, Vol Loss: 0.1422\n",
      "Validation loss did not improve. Patience: 4/100\n",
      "Epoch [107/2000], Total Loss: 428.0309, Change Loss: 427.6077, Valid Loss: 0.0055, Vol Loss: 0.1979\n",
      "Val - Total Loss: 443.4301, Change Loss: 443.1435, Valid Loss: 0.0000, Vol Loss: 0.1433\n",
      "Validation loss did not improve. Patience: 5/100\n",
      "Epoch [108/2000], Total Loss: 429.0829, Change Loss: 428.6784, Valid Loss: 0.0047, Vol Loss: 0.1906\n",
      "Val - Total Loss: 439.2304, Change Loss: 438.9679, Valid Loss: 0.0000, Vol Loss: 0.1313\n",
      "Validation loss did not improve. Patience: 6/100\n",
      "Epoch [110/2000], Total Loss: 427.5914, Change Loss: 427.2013, Valid Loss: 0.0040, Vol Loss: 0.1849\n",
      "Val - Total Loss: 436.4245, Change Loss: 436.1570, Valid Loss: 0.0000, Vol Loss: 0.1338\n",
      "Validation loss improved to 436.4245, saving model...\n",
      "Epoch [111/2000], Total Loss: 428.8293, Change Loss: 428.4308, Valid Loss: 0.0041, Vol Loss: 0.1889\n",
      "Val - Total Loss: 442.9991, Change Loss: 442.7167, Valid Loss: 0.0000, Vol Loss: 0.1412\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [112/2000], Total Loss: 428.2666, Change Loss: 427.8719, Valid Loss: 0.0049, Vol Loss: 0.1850\n",
      "Val - Total Loss: 436.1567, Change Loss: 435.8919, Valid Loss: 0.0000, Vol Loss: 0.1324\n",
      "Validation loss improved to 436.1567, saving model...\n",
      "Epoch [113/2000], Total Loss: 429.9100, Change Loss: 429.5141, Valid Loss: 0.0033, Vol Loss: 0.1897\n",
      "Val - Total Loss: 435.1711, Change Loss: 434.9038, Valid Loss: 0.0000, Vol Loss: 0.1337\n",
      "Validation loss improved to 435.1711, saving model...\n",
      "Epoch [114/2000], Total Loss: 427.6967, Change Loss: 427.2926, Valid Loss: 0.0049, Vol Loss: 0.1899\n",
      "Val - Total Loss: 440.4600, Change Loss: 440.1861, Valid Loss: 0.0000, Vol Loss: 0.1369\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [115/2000], Total Loss: 429.1412, Change Loss: 428.7377, Valid Loss: 0.0042, Vol Loss: 0.1913\n",
      "Val - Total Loss: 446.6164, Change Loss: 446.3325, Valid Loss: 0.0000, Vol Loss: 0.1419\n",
      "Validation loss did not improve. Patience: 2/100\n",
      "Epoch [116/2000], Total Loss: 428.2701, Change Loss: 427.8807, Valid Loss: 0.0052, Vol Loss: 0.1818\n",
      "Val - Total Loss: 443.1406, Change Loss: 442.8796, Valid Loss: 0.0000, Vol Loss: 0.1305\n",
      "Validation loss did not improve. Patience: 3/100\n",
      "Epoch [117/2000], Total Loss: 427.7843, Change Loss: 427.4145, Valid Loss: 0.0035, Vol Loss: 0.1762\n",
      "Val - Total Loss: 433.4728, Change Loss: 433.2016, Valid Loss: 0.0000, Vol Loss: 0.1356\n",
      "Validation loss improved to 433.4728, saving model...\n",
      "Epoch [118/2000], Total Loss: 426.5847, Change Loss: 426.2153, Valid Loss: 0.0032, Vol Loss: 0.1766\n",
      "Val - Total Loss: 434.4317, Change Loss: 434.1621, Valid Loss: 0.0000, Vol Loss: 0.1348\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [119/2000], Total Loss: 426.8973, Change Loss: 426.4967, Valid Loss: 0.0039, Vol Loss: 0.1906\n",
      "Val - Total Loss: 443.5290, Change Loss: 443.2550, Valid Loss: 0.0000, Vol Loss: 0.1370\n",
      "Validation loss did not improve. Patience: 2/100\n",
      "Epoch [120/2000], Total Loss: 427.9440, Change Loss: 427.5559, Valid Loss: 0.0040, Vol Loss: 0.1842\n",
      "Val - Total Loss: 439.8018, Change Loss: 439.5368, Valid Loss: 0.0000, Vol Loss: 0.1325\n",
      "Validation loss did not improve. Patience: 3/100\n",
      "Epoch [121/2000], Total Loss: 426.8248, Change Loss: 426.4273, Valid Loss: 0.0043, Vol Loss: 0.1881\n",
      "Val - Total Loss: 439.9638, Change Loss: 439.6892, Valid Loss: 0.0000, Vol Loss: 0.1373\n",
      "Validation loss did not improve. Patience: 4/100\n",
      "Epoch [122/2000], Total Loss: 427.6435, Change Loss: 427.2494, Valid Loss: 0.0045, Vol Loss: 0.1859\n",
      "Val - Total Loss: 436.5169, Change Loss: 436.2464, Valid Loss: 0.0000, Vol Loss: 0.1353\n",
      "Validation loss did not improve. Patience: 5/100\n",
      "Epoch [123/2000], Total Loss: 428.1658, Change Loss: 427.7720, Valid Loss: 0.0037, Vol Loss: 0.1878\n",
      "Val - Total Loss: 437.9529, Change Loss: 437.6819, Valid Loss: 0.0000, Vol Loss: 0.1355\n",
      "Validation loss did not improve. Patience: 6/100\n",
      "Epoch [124/2000], Total Loss: 427.4049, Change Loss: 427.0025, Valid Loss: 0.0039, Vol Loss: 0.1915\n",
      "Val - Total Loss: 439.0834, Change Loss: 438.8185, Valid Loss: 0.0000, Vol Loss: 0.1325\n",
      "Validation loss did not improve. Patience: 7/100\n",
      "Epoch [125/2000], Total Loss: 426.6663, Change Loss: 426.2625, Valid Loss: 0.0042, Vol Loss: 0.1914\n",
      "Val - Total Loss: 444.9342, Change Loss: 444.6654, Valid Loss: 0.0000, Vol Loss: 0.1344\n",
      "Validation loss did not improve. Patience: 8/100\n",
      "Epoch [126/2000], Total Loss: 427.4739, Change Loss: 427.0622, Valid Loss: 0.0049, Vol Loss: 0.1937\n",
      "Val - Total Loss: 438.8675, Change Loss: 438.5988, Valid Loss: 0.0000, Vol Loss: 0.1343\n",
      "Validation loss did not improve. Patience: 9/100\n",
      "Epoch [127/2000], Total Loss: 428.1749, Change Loss: 427.8095, Valid Loss: 0.0025, Vol Loss: 0.1765\n",
      "Val - Total Loss: 441.2679, Change Loss: 441.0172, Valid Loss: 0.0000, Vol Loss: 0.1253\n",
      "Validation loss did not improve. Patience: 10/100\n",
      "Epoch [128/2000], Total Loss: 425.1343, Change Loss: 424.7502, Valid Loss: 0.0044, Vol Loss: 0.1811\n",
      "Val - Total Loss: 443.2508, Change Loss: 442.9842, Valid Loss: 0.0000, Vol Loss: 0.1333\n",
      "Validation loss did not improve. Patience: 11/100\n",
      "Epoch [129/2000], Total Loss: 424.8289, Change Loss: 424.4231, Valid Loss: 0.0046, Vol Loss: 0.1915\n",
      "Val - Total Loss: 436.0963, Change Loss: 435.8323, Valid Loss: 0.0000, Vol Loss: 0.1320\n",
      "Validation loss did not improve. Patience: 12/100\n",
      "Epoch [130/2000], Total Loss: 426.2090, Change Loss: 425.8369, Valid Loss: 0.0029, Vol Loss: 0.1789\n",
      "Val - Total Loss: 436.5825, Change Loss: 436.3228, Valid Loss: 0.0000, Vol Loss: 0.1299\n",
      "Validation loss did not improve. Patience: 13/100\n",
      "Epoch [131/2000], Total Loss: 424.8111, Change Loss: 424.4186, Valid Loss: 0.0038, Vol Loss: 0.1869\n",
      "Val - Total Loss: 438.8937, Change Loss: 438.6065, Valid Loss: 0.0000, Vol Loss: 0.1436\n",
      "Validation loss did not improve. Patience: 14/100\n",
      "Epoch [132/2000], Total Loss: 425.9652, Change Loss: 425.5728, Valid Loss: 0.0038, Vol Loss: 0.1867\n",
      "Val - Total Loss: 437.1998, Change Loss: 436.9261, Valid Loss: 0.0000, Vol Loss: 0.1369\n",
      "Validation loss did not improve. Patience: 15/100\n",
      "Epoch [133/2000], Total Loss: 425.7866, Change Loss: 425.3977, Valid Loss: 0.0037, Vol Loss: 0.1852\n",
      "Val - Total Loss: 436.0394, Change Loss: 435.7475, Valid Loss: 0.0000, Vol Loss: 0.1460\n",
      "Validation loss did not improve. Patience: 16/100\n",
      "Epoch [134/2000], Total Loss: 425.7886, Change Loss: 425.3960, Valid Loss: 0.0036, Vol Loss: 0.1873\n",
      "Val - Total Loss: 435.4944, Change Loss: 435.2330, Valid Loss: 0.0000, Vol Loss: 0.1307\n",
      "Validation loss did not improve. Patience: 17/100\n",
      "Epoch [135/2000], Total Loss: 427.1913, Change Loss: 426.8013, Valid Loss: 0.0035, Vol Loss: 0.1864\n",
      "Val - Total Loss: 435.3868, Change Loss: 435.1254, Valid Loss: 0.0000, Vol Loss: 0.1307\n",
      "Validation loss did not improve. Patience: 18/100\n",
      "Epoch [136/2000], Total Loss: 425.2257, Change Loss: 424.8315, Valid Loss: 0.0046, Vol Loss: 0.1856\n",
      "Val - Total Loss: 437.0897, Change Loss: 436.8280, Valid Loss: 0.0000, Vol Loss: 0.1309\n",
      "Validation loss did not improve. Patience: 19/100\n",
      "Epoch [137/2000], Total Loss: 428.3623, Change Loss: 427.9846, Valid Loss: 0.0024, Vol Loss: 0.1828\n",
      "Val - Total Loss: 437.7257, Change Loss: 437.4677, Valid Loss: 0.0000, Vol Loss: 0.1290\n",
      "Validation loss did not improve. Patience: 20/100\n",
      "Epoch [138/2000], Total Loss: 425.8602, Change Loss: 425.4742, Valid Loss: 0.0040, Vol Loss: 0.1829\n",
      "Val - Total Loss: 441.1403, Change Loss: 440.8562, Valid Loss: 0.0000, Vol Loss: 0.1420\n",
      "Validation loss did not improve. Patience: 21/100\n",
      "Epoch [139/2000], Total Loss: 426.5424, Change Loss: 426.1547, Valid Loss: 0.0042, Vol Loss: 0.1834\n",
      "Val - Total Loss: 437.4693, Change Loss: 437.1885, Valid Loss: 0.0000, Vol Loss: 0.1404\n",
      "Validation loss did not improve. Patience: 22/100\n",
      "Epoch [140/2000], Total Loss: 425.2776, Change Loss: 424.8993, Valid Loss: 0.0033, Vol Loss: 0.1809\n",
      "Val - Total Loss: 437.3253, Change Loss: 437.0446, Valid Loss: 0.0000, Vol Loss: 0.1404\n",
      "Validation loss did not improve. Patience: 23/100\n",
      "Epoch [141/2000], Total Loss: 428.7140, Change Loss: 428.3241, Valid Loss: 0.0036, Vol Loss: 0.1860\n",
      "Val - Total Loss: 437.2202, Change Loss: 436.9470, Valid Loss: 0.0000, Vol Loss: 0.1366\n",
      "Validation loss did not improve. Patience: 24/100\n",
      "Epoch [142/2000], Total Loss: 424.3938, Change Loss: 423.9979, Valid Loss: 0.0045, Vol Loss: 0.1867\n",
      "Val - Total Loss: 438.1965, Change Loss: 437.9312, Valid Loss: 0.0000, Vol Loss: 0.1326\n",
      "Validation loss did not improve. Patience: 25/100\n",
      "Epoch [143/2000], Total Loss: 426.2371, Change Loss: 425.8582, Valid Loss: 0.0028, Vol Loss: 0.1823\n",
      "Val - Total Loss: 436.0307, Change Loss: 435.7609, Valid Loss: 0.0000, Vol Loss: 0.1349\n",
      "Validation loss did not improve. Patience: 26/100\n",
      "Epoch [144/2000], Total Loss: 427.6985, Change Loss: 427.3144, Valid Loss: 0.0034, Vol Loss: 0.1835\n",
      "Val - Total Loss: 436.7626, Change Loss: 436.4991, Valid Loss: 0.0000, Vol Loss: 0.1317\n",
      "Validation loss did not improve. Patience: 27/100\n",
      "Epoch [145/2000], Total Loss: 426.3957, Change Loss: 426.0141, Valid Loss: 0.0043, Vol Loss: 0.1801\n",
      "Val - Total Loss: 438.3600, Change Loss: 438.0972, Valid Loss: 0.0000, Vol Loss: 0.1314\n",
      "Validation loss did not improve. Patience: 28/100\n",
      "Epoch [146/2000], Total Loss: 425.2940, Change Loss: 424.9121, Valid Loss: 0.0034, Vol Loss: 0.1826\n",
      "Val - Total Loss: 439.5538, Change Loss: 439.2790, Valid Loss: 0.0000, Vol Loss: 0.1374\n",
      "Validation loss did not improve. Patience: 29/100\n",
      "Epoch [147/2000], Total Loss: 426.0841, Change Loss: 425.6945, Valid Loss: 0.0026, Vol Loss: 0.1882\n",
      "Val - Total Loss: 439.4244, Change Loss: 439.1531, Valid Loss: 0.0000, Vol Loss: 0.1356\n",
      "Validation loss did not improve. Patience: 30/100\n",
      "Epoch [148/2000], Total Loss: 425.8373, Change Loss: 425.4517, Valid Loss: 0.0032, Vol Loss: 0.1849\n",
      "Val - Total Loss: 439.0974, Change Loss: 438.8297, Valid Loss: 0.0000, Vol Loss: 0.1339\n",
      "Validation loss did not improve. Patience: 31/100\n",
      "Epoch [149/2000], Total Loss: 426.0027, Change Loss: 425.6269, Valid Loss: 0.0034, Vol Loss: 0.1794\n",
      "Val - Total Loss: 446.6821, Change Loss: 446.3964, Valid Loss: 0.0000, Vol Loss: 0.1428\n",
      "Validation loss did not improve. Patience: 32/100\n",
      "Epoch [150/2000], Total Loss: 423.8210, Change Loss: 423.4317, Valid Loss: 0.0045, Vol Loss: 0.1833\n",
      "Val - Total Loss: 442.6021, Change Loss: 442.3221, Valid Loss: 0.0000, Vol Loss: 0.1400\n",
      "Validation loss did not improve. Patience: 33/100\n",
      "Epoch [151/2000], Total Loss: 424.1982, Change Loss: 423.8241, Valid Loss: 0.0035, Vol Loss: 0.1784\n",
      "Val - Total Loss: 443.4416, Change Loss: 443.1640, Valid Loss: 0.0000, Vol Loss: 0.1388\n",
      "Validation loss did not improve. Patience: 34/100\n",
      "Epoch [152/2000], Total Loss: 425.9030, Change Loss: 425.5159, Valid Loss: 0.0036, Vol Loss: 0.1845\n",
      "Val - Total Loss: 439.8832, Change Loss: 439.6023, Valid Loss: 0.0000, Vol Loss: 0.1404\n",
      "Validation loss did not improve. Patience: 35/100\n",
      "Epoch [153/2000], Total Loss: 426.0973, Change Loss: 425.7162, Valid Loss: 0.0035, Vol Loss: 0.1817\n",
      "Val - Total Loss: 435.2325, Change Loss: 434.9634, Valid Loss: 0.0000, Vol Loss: 0.1346\n",
      "Validation loss did not improve. Patience: 36/100\n",
      "Epoch [154/2000], Total Loss: 424.5371, Change Loss: 424.1560, Valid Loss: 0.0032, Vol Loss: 0.1827\n",
      "Val - Total Loss: 433.8281, Change Loss: 433.5600, Valid Loss: 0.0000, Vol Loss: 0.1341\n",
      "Validation loss did not improve. Patience: 37/100\n",
      "Epoch [155/2000], Total Loss: 423.4590, Change Loss: 423.0724, Valid Loss: 0.0034, Vol Loss: 0.1847\n",
      "Val - Total Loss: 435.2496, Change Loss: 434.9766, Valid Loss: 0.0000, Vol Loss: 0.1365\n",
      "Validation loss did not improve. Patience: 38/100\n",
      "Epoch [156/2000], Total Loss: 424.5820, Change Loss: 424.1813, Valid Loss: 0.0033, Vol Loss: 0.1920\n",
      "Val - Total Loss: 444.2415, Change Loss: 443.9626, Valid Loss: 0.0000, Vol Loss: 0.1394\n",
      "Validation loss did not improve. Patience: 39/100\n",
      "Epoch [157/2000], Total Loss: 425.8475, Change Loss: 425.4754, Valid Loss: 0.0028, Vol Loss: 0.1790\n",
      "Val - Total Loss: 445.0947, Change Loss: 444.8249, Valid Loss: 0.0000, Vol Loss: 0.1349\n",
      "Validation loss did not improve. Patience: 40/100\n",
      "Epoch [158/2000], Total Loss: 424.9247, Change Loss: 424.5450, Valid Loss: 0.0029, Vol Loss: 0.1826\n",
      "Val - Total Loss: 436.5430, Change Loss: 436.2592, Valid Loss: 0.0000, Vol Loss: 0.1419\n",
      "Validation loss did not improve. Patience: 41/100\n",
      "Epoch [159/2000], Total Loss: 425.5027, Change Loss: 425.1025, Valid Loss: 0.0034, Vol Loss: 0.1916\n",
      "Val - Total Loss: 442.2329, Change Loss: 441.9543, Valid Loss: 0.0000, Vol Loss: 0.1393\n",
      "Validation loss did not improve. Patience: 42/100\n",
      "Epoch [160/2000], Total Loss: 423.3233, Change Loss: 422.9448, Valid Loss: 0.0037, Vol Loss: 0.1799\n",
      "Val - Total Loss: 452.4457, Change Loss: 452.1433, Valid Loss: 0.0000, Vol Loss: 0.1512\n",
      "Validation loss did not improve. Patience: 43/100\n",
      "Epoch [161/2000], Total Loss: 424.9057, Change Loss: 424.5239, Valid Loss: 0.0037, Vol Loss: 0.1818\n",
      "Val - Total Loss: 437.8518, Change Loss: 437.5870, Valid Loss: 0.0000, Vol Loss: 0.1324\n",
      "Validation loss did not improve. Patience: 44/100\n",
      "Epoch [162/2000], Total Loss: 424.5707, Change Loss: 424.1723, Valid Loss: 0.0035, Vol Loss: 0.1904\n",
      "Val - Total Loss: 439.7961, Change Loss: 439.5091, Valid Loss: 0.0000, Vol Loss: 0.1435\n",
      "Validation loss did not improve. Patience: 45/100\n",
      "Epoch [163/2000], Total Loss: 423.5518, Change Loss: 423.1736, Valid Loss: 0.0026, Vol Loss: 0.1827\n",
      "Val - Total Loss: 442.1719, Change Loss: 441.8953, Valid Loss: 0.0000, Vol Loss: 0.1383\n",
      "Validation loss did not improve. Patience: 46/100\n",
      "Epoch [164/2000], Total Loss: 424.0558, Change Loss: 423.6627, Valid Loss: 0.0040, Vol Loss: 0.1866\n",
      "Val - Total Loss: 439.1871, Change Loss: 438.9072, Valid Loss: 0.0000, Vol Loss: 0.1400\n",
      "Validation loss did not improve. Patience: 47/100\n",
      "Epoch [165/2000], Total Loss: 425.0914, Change Loss: 424.7088, Valid Loss: 0.0030, Vol Loss: 0.1839\n",
      "Val - Total Loss: 442.5955, Change Loss: 442.3224, Valid Loss: 0.0000, Vol Loss: 0.1365\n",
      "Validation loss did not improve. Patience: 48/100\n",
      "Epoch [166/2000], Total Loss: 422.2670, Change Loss: 421.8810, Valid Loss: 0.0031, Vol Loss: 0.1853\n",
      "Val - Total Loss: 445.1067, Change Loss: 444.8398, Valid Loss: 0.0000, Vol Loss: 0.1335\n",
      "Validation loss did not improve. Patience: 49/100\n",
      "Epoch [167/2000], Total Loss: 423.5806, Change Loss: 423.1984, Valid Loss: 0.0031, Vol Loss: 0.1833\n",
      "Val - Total Loss: 442.1385, Change Loss: 441.8891, Valid Loss: 0.0000, Vol Loss: 0.1247\n",
      "Validation loss did not improve. Patience: 50/100\n",
      "Epoch [168/2000], Total Loss: 423.8066, Change Loss: 423.4341, Valid Loss: 0.0028, Vol Loss: 0.1794\n",
      "Val - Total Loss: 442.7770, Change Loss: 442.5230, Valid Loss: 0.0000, Vol Loss: 0.1270\n",
      "Validation loss did not improve. Patience: 51/100\n",
      "Epoch [169/2000], Total Loss: 423.9729, Change Loss: 423.6058, Valid Loss: 0.0025, Vol Loss: 0.1773\n",
      "Val - Total Loss: 447.7562, Change Loss: 447.4930, Valid Loss: 0.0000, Vol Loss: 0.1316\n",
      "Validation loss did not improve. Patience: 52/100\n",
      "Epoch [170/2000], Total Loss: 422.9319, Change Loss: 422.5391, Valid Loss: 0.0033, Vol Loss: 0.1881\n",
      "Val - Total Loss: 447.6032, Change Loss: 447.3469, Valid Loss: 0.0000, Vol Loss: 0.1282\n",
      "Validation loss did not improve. Patience: 53/100\n",
      "Epoch [171/2000], Total Loss: 424.9732, Change Loss: 424.5999, Valid Loss: 0.0029, Vol Loss: 0.1795\n",
      "Val - Total Loss: 442.7424, Change Loss: 442.4944, Valid Loss: 0.0000, Vol Loss: 0.1240\n",
      "Validation loss did not improve. Patience: 54/100\n",
      "Epoch [172/2000], Total Loss: 422.1120, Change Loss: 421.7158, Valid Loss: 0.0036, Vol Loss: 0.1891\n",
      "Val - Total Loss: 443.6321, Change Loss: 443.3816, Valid Loss: 0.0000, Vol Loss: 0.1253\n",
      "Validation loss did not improve. Patience: 55/100\n",
      "Epoch [173/2000], Total Loss: 425.6944, Change Loss: 425.3042, Valid Loss: 0.0031, Vol Loss: 0.1872\n",
      "Val - Total Loss: 437.0654, Change Loss: 436.8159, Valid Loss: 0.0000, Vol Loss: 0.1248\n",
      "Validation loss did not improve. Patience: 56/100\n",
      "Epoch [174/2000], Total Loss: 423.2105, Change Loss: 422.8320, Valid Loss: 0.0030, Vol Loss: 0.1818\n",
      "Val - Total Loss: 435.2524, Change Loss: 435.0103, Valid Loss: 0.0000, Vol Loss: 0.1210\n",
      "Validation loss did not improve. Patience: 57/100\n",
      "Epoch [176/2000], Total Loss: 422.0282, Change Loss: 421.6496, Valid Loss: 0.0025, Vol Loss: 0.1830\n",
      "Val - Total Loss: 442.5324, Change Loss: 442.2744, Valid Loss: 0.0000, Vol Loss: 0.1290\n",
      "Validation loss did not improve. Patience: 59/100\n",
      "Epoch [177/2000], Total Loss: 424.2895, Change Loss: 423.8955, Valid Loss: 0.0026, Vol Loss: 0.1905\n",
      "Val - Total Loss: 442.0654, Change Loss: 441.8146, Valid Loss: 0.0000, Vol Loss: 0.1254\n",
      "Validation loss did not improve. Patience: 60/100\n",
      "Epoch [178/2000], Total Loss: 422.1832, Change Loss: 421.8052, Valid Loss: 0.0030, Vol Loss: 0.1815\n",
      "Val - Total Loss: 439.8379, Change Loss: 439.5823, Valid Loss: 0.0000, Vol Loss: 0.1278\n",
      "Validation loss did not improve. Patience: 61/100\n",
      "Epoch [180/2000], Total Loss: 425.1076, Change Loss: 424.7152, Valid Loss: 0.0032, Vol Loss: 0.1883\n",
      "Val - Total Loss: 441.9866, Change Loss: 441.7068, Valid Loss: 0.0000, Vol Loss: 0.1399\n",
      "Validation loss did not improve. Patience: 63/100\n",
      "Epoch [181/2000], Total Loss: 425.9242, Change Loss: 425.5298, Valid Loss: 0.0034, Vol Loss: 0.1888\n",
      "Val - Total Loss: 442.6490, Change Loss: 442.3760, Valid Loss: 0.0000, Vol Loss: 0.1365\n",
      "Validation loss did not improve. Patience: 64/100\n",
      "Epoch [182/2000], Total Loss: 426.1544, Change Loss: 425.7816, Valid Loss: 0.0024, Vol Loss: 0.1805\n",
      "Val - Total Loss: 442.9693, Change Loss: 442.6954, Valid Loss: 0.0000, Vol Loss: 0.1370\n",
      "Validation loss did not improve. Patience: 65/100\n",
      "Epoch [183/2000], Total Loss: 423.8153, Change Loss: 423.4497, Valid Loss: 0.0022, Vol Loss: 0.1773\n",
      "Val - Total Loss: 435.8784, Change Loss: 435.6245, Valid Loss: 0.0000, Vol Loss: 0.1269\n",
      "Validation loss did not improve. Patience: 66/100\n",
      "Epoch [184/2000], Total Loss: 424.2035, Change Loss: 423.8424, Valid Loss: 0.0027, Vol Loss: 0.1739\n",
      "Val - Total Loss: 438.1921, Change Loss: 437.9231, Valid Loss: 0.0000, Vol Loss: 0.1345\n",
      "Validation loss did not improve. Patience: 67/100\n",
      "Epoch [185/2000], Total Loss: 423.3060, Change Loss: 422.9054, Valid Loss: 0.0030, Vol Loss: 0.1929\n",
      "Val - Total Loss: 439.0291, Change Loss: 438.7666, Valid Loss: 0.0000, Vol Loss: 0.1312\n",
      "Validation loss did not improve. Patience: 68/100\n",
      "Epoch [186/2000], Total Loss: 426.0562, Change Loss: 425.6669, Valid Loss: 0.0030, Vol Loss: 0.1872\n",
      "Val - Total Loss: 440.3505, Change Loss: 440.0862, Valid Loss: 0.0000, Vol Loss: 0.1321\n",
      "Validation loss did not improve. Patience: 69/100\n",
      "Epoch [187/2000], Total Loss: 424.9384, Change Loss: 424.5621, Valid Loss: 0.0030, Vol Loss: 0.1807\n",
      "Val - Total Loss: 437.8139, Change Loss: 437.5522, Valid Loss: 0.0000, Vol Loss: 0.1309\n",
      "Validation loss did not improve. Patience: 70/100\n",
      "Epoch [188/2000], Total Loss: 425.1755, Change Loss: 424.7878, Valid Loss: 0.0021, Vol Loss: 0.1885\n",
      "Val - Total Loss: 448.4981, Change Loss: 448.2447, Valid Loss: 0.0000, Vol Loss: 0.1267\n",
      "Validation loss did not improve. Patience: 71/100\n",
      "Epoch [189/2000], Total Loss: 423.0547, Change Loss: 422.6743, Valid Loss: 0.0032, Vol Loss: 0.1822\n",
      "Val - Total Loss: 439.9744, Change Loss: 439.7143, Valid Loss: 0.0000, Vol Loss: 0.1300\n",
      "Validation loss did not improve. Patience: 72/100\n",
      "Epoch [190/2000], Total Loss: 422.7594, Change Loss: 422.3694, Valid Loss: 0.0029, Vol Loss: 0.1876\n",
      "Val - Total Loss: 442.9784, Change Loss: 442.7189, Valid Loss: 0.0000, Vol Loss: 0.1297\n",
      "Validation loss did not improve. Patience: 73/100\n",
      "Epoch [191/2000], Total Loss: 421.7128, Change Loss: 421.3183, Valid Loss: 0.0029, Vol Loss: 0.1900\n",
      "Val - Total Loss: 442.4567, Change Loss: 442.1998, Valid Loss: 0.0000, Vol Loss: 0.1285\n",
      "Validation loss did not improve. Patience: 74/100\n",
      "Epoch [193/2000], Total Loss: 423.0409, Change Loss: 422.6327, Valid Loss: 0.0034, Vol Loss: 0.1956\n",
      "Val - Total Loss: 443.2423, Change Loss: 442.9720, Valid Loss: 0.0000, Vol Loss: 0.1351\n",
      "Validation loss did not improve. Patience: 76/100\n",
      "Epoch [194/2000], Total Loss: 422.2722, Change Loss: 421.8878, Valid Loss: 0.0025, Vol Loss: 0.1858\n",
      "Val - Total Loss: 442.8538, Change Loss: 442.6050, Valid Loss: 0.0000, Vol Loss: 0.1244\n",
      "Validation loss did not improve. Patience: 77/100\n",
      "Epoch [195/2000], Total Loss: 425.1018, Change Loss: 424.7309, Valid Loss: 0.0023, Vol Loss: 0.1796\n",
      "Val - Total Loss: 449.6928, Change Loss: 449.4433, Valid Loss: 0.0000, Vol Loss: 0.1247\n",
      "Validation loss did not improve. Patience: 78/100\n",
      "Epoch [196/2000], Total Loss: 424.1133, Change Loss: 423.7392, Valid Loss: 0.0025, Vol Loss: 0.1807\n",
      "Val - Total Loss: 445.7447, Change Loss: 445.5049, Valid Loss: 0.0000, Vol Loss: 0.1199\n",
      "Validation loss did not improve. Patience: 79/100\n",
      "Epoch [197/2000], Total Loss: 421.1306, Change Loss: 420.7530, Valid Loss: 0.0023, Vol Loss: 0.1830\n",
      "Val - Total Loss: 443.4098, Change Loss: 443.1421, Valid Loss: 0.0000, Vol Loss: 0.1339\n",
      "Validation loss did not improve. Patience: 80/100\n",
      "Epoch [198/2000], Total Loss: 422.2565, Change Loss: 421.8634, Valid Loss: 0.0031, Vol Loss: 0.1887\n",
      "Val - Total Loss: 440.0824, Change Loss: 439.8282, Valid Loss: 0.0000, Vol Loss: 0.1271\n",
      "Validation loss did not improve. Patience: 81/100\n",
      "Epoch [199/2000], Total Loss: 425.0831, Change Loss: 424.7007, Valid Loss: 0.0029, Vol Loss: 0.1840\n",
      "Val - Total Loss: 448.9504, Change Loss: 448.6925, Valid Loss: 0.0000, Vol Loss: 0.1290\n",
      "Validation loss did not improve. Patience: 82/100\n",
      "Epoch [200/2000], Total Loss: 423.6368, Change Loss: 423.2500, Valid Loss: 0.0023, Vol Loss: 0.1876\n",
      "Val - Total Loss: 441.5832, Change Loss: 441.3275, Valid Loss: 0.0000, Vol Loss: 0.1278\n",
      "Validation loss did not improve. Patience: 83/100\n",
      "Epoch [201/2000], Total Loss: 423.1052, Change Loss: 422.7394, Valid Loss: 0.0020, Vol Loss: 0.1778\n",
      "Val - Total Loss: 443.0191, Change Loss: 442.7857, Valid Loss: 0.0000, Vol Loss: 0.1167\n",
      "Validation loss did not improve. Patience: 84/100\n",
      "Epoch [202/2000], Total Loss: 422.3488, Change Loss: 421.9896, Valid Loss: 0.0024, Vol Loss: 0.1736\n",
      "Val - Total Loss: 440.3210, Change Loss: 440.0728, Valid Loss: 0.0000, Vol Loss: 0.1241\n",
      "Validation loss did not improve. Patience: 85/100\n",
      "Epoch [203/2000], Total Loss: 421.1155, Change Loss: 420.7235, Valid Loss: 0.0020, Vol Loss: 0.1911\n",
      "Val - Total Loss: 454.5670, Change Loss: 454.3066, Valid Loss: 0.0000, Vol Loss: 0.1302\n",
      "Validation loss did not improve. Patience: 86/100\n",
      "Epoch [204/2000], Total Loss: 421.1069, Change Loss: 420.7278, Valid Loss: 0.0028, Vol Loss: 0.1826\n",
      "Val - Total Loss: 457.3216, Change Loss: 457.0779, Valid Loss: 0.0000, Vol Loss: 0.1218\n",
      "Validation loss did not improve. Patience: 87/100\n",
      "Epoch [205/2000], Total Loss: 424.2146, Change Loss: 423.8545, Valid Loss: 0.0017, Vol Loss: 0.1757\n",
      "Val - Total Loss: 445.3992, Change Loss: 445.1549, Valid Loss: 0.0000, Vol Loss: 0.1222\n",
      "Validation loss did not improve. Patience: 88/100\n",
      "Epoch [206/2000], Total Loss: 423.2115, Change Loss: 422.8119, Valid Loss: 0.0029, Vol Loss: 0.1924\n",
      "Val - Total Loss: 447.7524, Change Loss: 447.4908, Valid Loss: 0.0000, Vol Loss: 0.1308\n",
      "Validation loss did not improve. Patience: 89/100\n",
      "Epoch [208/2000], Total Loss: 421.9176, Change Loss: 421.5489, Valid Loss: 0.0023, Vol Loss: 0.1785\n",
      "Val - Total Loss: 439.4635, Change Loss: 439.2095, Valid Loss: 0.0000, Vol Loss: 0.1270\n",
      "Validation loss did not improve. Patience: 91/100\n",
      "Epoch [209/2000], Total Loss: 422.3676, Change Loss: 421.9858, Valid Loss: 0.0025, Vol Loss: 0.1847\n",
      "Val - Total Loss: 443.6816, Change Loss: 443.4201, Valid Loss: 0.0000, Vol Loss: 0.1307\n",
      "Validation loss did not improve. Patience: 92/100\n",
      "Epoch [210/2000], Total Loss: 422.0051, Change Loss: 421.6249, Valid Loss: 0.0026, Vol Loss: 0.1835\n",
      "Val - Total Loss: 446.8899, Change Loss: 446.6306, Valid Loss: 0.0000, Vol Loss: 0.1297\n",
      "Validation loss did not improve. Patience: 93/100\n",
      "Epoch [211/2000], Total Loss: 420.5459, Change Loss: 420.1748, Valid Loss: 0.0020, Vol Loss: 0.1806\n",
      "Val - Total Loss: 446.2217, Change Loss: 445.9749, Valid Loss: 0.0000, Vol Loss: 0.1234\n",
      "Validation loss did not improve. Patience: 94/100\n",
      "Epoch [213/2000], Total Loss: 424.0206, Change Loss: 423.6312, Valid Loss: 0.0032, Vol Loss: 0.1867\n",
      "Val - Total Loss: 433.3807, Change Loss: 433.1354, Valid Loss: 0.0000, Vol Loss: 0.1227\n",
      "Validation loss improved to 433.3807, saving model...\n",
      "Epoch [214/2000], Total Loss: 420.5519, Change Loss: 420.1774, Valid Loss: 0.0027, Vol Loss: 0.1805\n",
      "Val - Total Loss: 438.8492, Change Loss: 438.6052, Valid Loss: 0.0000, Vol Loss: 0.1220\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [215/2000], Total Loss: 421.2096, Change Loss: 420.8401, Valid Loss: 0.0021, Vol Loss: 0.1795\n",
      "Val - Total Loss: 443.0137, Change Loss: 442.7627, Valid Loss: 0.0000, Vol Loss: 0.1255\n",
      "Validation loss did not improve. Patience: 2/100\n",
      "Epoch [216/2000], Total Loss: 422.4838, Change Loss: 422.1100, Valid Loss: 0.0024, Vol Loss: 0.1809\n",
      "Val - Total Loss: 441.9187, Change Loss: 441.6695, Valid Loss: 0.0000, Vol Loss: 0.1246\n",
      "Validation loss did not improve. Patience: 3/100\n",
      "Epoch [217/2000], Total Loss: 419.6790, Change Loss: 419.3141, Valid Loss: 0.0018, Vol Loss: 0.1780\n",
      "Val - Total Loss: 446.7683, Change Loss: 446.5099, Valid Loss: 0.0000, Vol Loss: 0.1292\n",
      "Validation loss did not improve. Patience: 4/100\n",
      "Epoch [218/2000], Total Loss: 423.2896, Change Loss: 422.8936, Valid Loss: 0.0025, Vol Loss: 0.1918\n",
      "Val - Total Loss: 429.4459, Change Loss: 429.1816, Valid Loss: 0.0000, Vol Loss: 0.1322\n",
      "Validation loss improved to 429.4459, saving model...\n",
      "Epoch [219/2000], Total Loss: 418.3786, Change Loss: 417.9894, Valid Loss: 0.0029, Vol Loss: 0.1874\n",
      "Val - Total Loss: 439.5380, Change Loss: 439.2681, Valid Loss: 0.0000, Vol Loss: 0.1349\n",
      "Validation loss did not improve. Patience: 1/100\n",
      "Epoch [220/2000], Total Loss: 421.6100, Change Loss: 421.2285, Valid Loss: 0.0025, Vol Loss: 0.1844\n",
      "Val - Total Loss: 433.1161, Change Loss: 432.8599, Valid Loss: 0.0000, Vol Loss: 0.1281\n",
      "Validation loss did not improve. Patience: 2/100\n",
      "Epoch [221/2000], Total Loss: 421.8986, Change Loss: 421.5414, Valid Loss: 0.0019, Vol Loss: 0.1739\n",
      "Val - Total Loss: 443.6181, Change Loss: 443.3513, Valid Loss: 0.0000, Vol Loss: 0.1334\n",
      "Validation loss did not improve. Patience: 3/100\n",
      "Epoch [222/2000], Total Loss: 418.3140, Change Loss: 417.9170, Valid Loss: 0.0028, Vol Loss: 0.1915\n",
      "Val - Total Loss: 441.2841, Change Loss: 440.9861, Valid Loss: 0.0000, Vol Loss: 0.1490\n",
      "Validation loss did not improve. Patience: 4/100\n",
      "Epoch [223/2000], Total Loss: 418.8596, Change Loss: 418.4575, Valid Loss: 0.0023, Vol Loss: 0.1954\n",
      "Val - Total Loss: 449.1734, Change Loss: 448.9024, Valid Loss: 0.0000, Vol Loss: 0.1355\n",
      "Validation loss did not improve. Patience: 5/100\n",
      "Epoch [224/2000], Total Loss: 419.3982, Change Loss: 419.0187, Valid Loss: 0.0024, Vol Loss: 0.1838\n",
      "Val - Total Loss: 445.9263, Change Loss: 445.6666, Valid Loss: 0.0000, Vol Loss: 0.1298\n",
      "Validation loss did not improve. Patience: 6/100\n",
      "Epoch [225/2000], Total Loss: 420.4847, Change Loss: 420.1083, Valid Loss: 0.0021, Vol Loss: 0.1829\n",
      "Val - Total Loss: 447.5124, Change Loss: 447.2598, Valid Loss: 0.0000, Vol Loss: 0.1263\n",
      "Validation loss did not improve. Patience: 7/100\n",
      "Epoch [226/2000], Total Loss: 421.0610, Change Loss: 420.6811, Valid Loss: 0.0022, Vol Loss: 0.1844\n",
      "Val - Total Loss: 443.5872, Change Loss: 443.3323, Valid Loss: 0.0000, Vol Loss: 0.1275\n",
      "Validation loss did not improve. Patience: 8/100\n",
      "Epoch [227/2000], Total Loss: 417.5682, Change Loss: 417.1978, Valid Loss: 0.0027, Vol Loss: 0.1785\n",
      "Val - Total Loss: 448.0378, Change Loss: 447.7455, Valid Loss: 0.0000, Vol Loss: 0.1462\n",
      "Validation loss did not improve. Patience: 9/100\n",
      "Epoch [228/2000], Total Loss: 418.0449, Change Loss: 417.6521, Valid Loss: 0.0027, Vol Loss: 0.1896\n",
      "Val - Total Loss: 437.7097, Change Loss: 437.4426, Valid Loss: 0.0000, Vol Loss: 0.1336\n",
      "Validation loss did not improve. Patience: 10/100\n",
      "Epoch [229/2000], Total Loss: 419.4909, Change Loss: 419.1170, Valid Loss: 0.0029, Vol Loss: 0.1797\n",
      "Val - Total Loss: 447.6296, Change Loss: 447.3605, Valid Loss: 0.0000, Vol Loss: 0.1346\n",
      "Validation loss did not improve. Patience: 11/100\n",
      "Epoch [230/2000], Total Loss: 421.4996, Change Loss: 421.1125, Valid Loss: 0.0022, Vol Loss: 0.1881\n",
      "Val - Total Loss: 447.2258, Change Loss: 446.9423, Valid Loss: 0.0000, Vol Loss: 0.1418\n",
      "Validation loss did not improve. Patience: 12/100\n",
      "Epoch [231/2000], Total Loss: 418.7080, Change Loss: 418.3126, Valid Loss: 0.0023, Vol Loss: 0.1919\n",
      "Val - Total Loss: 449.7303, Change Loss: 449.4637, Valid Loss: 0.0000, Vol Loss: 0.1333\n",
      "Validation loss did not improve. Patience: 13/100\n",
      "Epoch [232/2000], Total Loss: 421.0209, Change Loss: 420.6539, Valid Loss: 0.0022, Vol Loss: 0.1780\n",
      "Val - Total Loss: 443.7523, Change Loss: 443.4973, Valid Loss: 0.0000, Vol Loss: 0.1275\n",
      "Validation loss did not improve. Patience: 14/100\n",
      "Epoch [233/2000], Total Loss: 417.9509, Change Loss: 417.5720, Valid Loss: 0.0016, Vol Loss: 0.1856\n",
      "Val - Total Loss: 442.9290, Change Loss: 442.6742, Valid Loss: 0.0000, Vol Loss: 0.1274\n",
      "Validation loss did not improve. Patience: 15/100\n",
      "Epoch [234/2000], Total Loss: 419.3229, Change Loss: 418.9519, Valid Loss: 0.0022, Vol Loss: 0.1800\n",
      "Val - Total Loss: 443.3554, Change Loss: 443.0906, Valid Loss: 0.0000, Vol Loss: 0.1324\n",
      "Validation loss did not improve. Patience: 16/100\n",
      "Epoch [235/2000], Total Loss: 419.4287, Change Loss: 419.0624, Valid Loss: 0.0015, Vol Loss: 0.1795\n",
      "Val - Total Loss: 440.5178, Change Loss: 440.2456, Valid Loss: 0.0000, Vol Loss: 0.1361\n",
      "Validation loss did not improve. Patience: 17/100\n",
      "Epoch [236/2000], Total Loss: 420.0914, Change Loss: 419.7229, Valid Loss: 0.0023, Vol Loss: 0.1784\n",
      "Val - Total Loss: 444.7749, Change Loss: 444.5181, Valid Loss: 0.0000, Vol Loss: 0.1284\n",
      "Validation loss did not improve. Patience: 18/100\n",
      "Epoch [237/2000], Total Loss: 416.9139, Change Loss: 416.5260, Valid Loss: 0.0021, Vol Loss: 0.1887\n",
      "Val - Total Loss: 439.5840, Change Loss: 439.3299, Valid Loss: 0.0000, Vol Loss: 0.1271\n",
      "Validation loss did not improve. Patience: 19/100\n",
      "Epoch [238/2000], Total Loss: 419.5160, Change Loss: 419.1461, Valid Loss: 0.0016, Vol Loss: 0.1811\n",
      "Val - Total Loss: 438.4817, Change Loss: 438.2115, Valid Loss: 0.0000, Vol Loss: 0.1351\n",
      "Validation loss did not improve. Patience: 20/100\n",
      "Epoch [239/2000], Total Loss: 419.6550, Change Loss: 419.2737, Valid Loss: 0.0018, Vol Loss: 0.1861\n",
      "Val - Total Loss: 445.2534, Change Loss: 444.9939, Valid Loss: 0.0000, Vol Loss: 0.1297\n",
      "Validation loss did not improve. Patience: 21/100\n",
      "Epoch [241/2000], Total Loss: 418.5857, Change Loss: 418.1906, Valid Loss: 0.0024, Vol Loss: 0.1915\n",
      "Val - Total Loss: 442.7675, Change Loss: 442.4848, Valid Loss: 0.0000, Vol Loss: 0.1414\n",
      "Validation loss did not improve. Patience: 23/100\n",
      "Epoch [242/2000], Total Loss: 421.2416, Change Loss: 420.8803, Valid Loss: 0.0010, Vol Loss: 0.1780\n",
      "Val - Total Loss: 448.5721, Change Loss: 448.3169, Valid Loss: 0.0000, Vol Loss: 0.1276\n",
      "Validation loss did not improve. Patience: 24/100\n",
      "Epoch [243/2000], Total Loss: 420.6939, Change Loss: 420.3220, Valid Loss: 0.0023, Vol Loss: 0.1802\n",
      "Val - Total Loss: 446.4321, Change Loss: 446.1653, Valid Loss: 0.0000, Vol Loss: 0.1334\n",
      "Validation loss did not improve. Patience: 25/100\n",
      "Epoch [244/2000], Total Loss: 419.7500, Change Loss: 419.3611, Valid Loss: 0.0019, Vol Loss: 0.1898\n",
      "Val - Total Loss: 441.0260, Change Loss: 440.7630, Valid Loss: 0.0000, Vol Loss: 0.1315\n",
      "Validation loss did not improve. Patience: 26/100\n",
      "Epoch [245/2000], Total Loss: 421.1991, Change Loss: 420.8385, Valid Loss: 0.0016, Vol Loss: 0.1763\n",
      "Val - Total Loss: 440.9543, Change Loss: 440.6895, Valid Loss: 0.0000, Vol Loss: 0.1324\n",
      "Validation loss did not improve. Patience: 27/100\n",
      "Epoch [246/2000], Total Loss: 418.4160, Change Loss: 418.0461, Valid Loss: 0.0014, Vol Loss: 0.1814\n",
      "Val - Total Loss: 444.6748, Change Loss: 444.4100, Valid Loss: 0.0000, Vol Loss: 0.1324\n",
      "Validation loss did not improve. Patience: 28/100\n",
      "Epoch [247/2000], Total Loss: 420.0150, Change Loss: 419.6348, Valid Loss: 0.0018, Vol Loss: 0.1857\n",
      "Val - Total Loss: 444.5378, Change Loss: 444.2661, Valid Loss: 0.0000, Vol Loss: 0.1359\n",
      "Validation loss did not improve. Patience: 29/100\n",
      "Epoch [248/2000], Total Loss: 417.8359, Change Loss: 417.4677, Valid Loss: 0.0022, Vol Loss: 0.1785\n",
      "Val - Total Loss: 443.0957, Change Loss: 442.8343, Valid Loss: 0.0000, Vol Loss: 0.1307\n",
      "Validation loss did not improve. Patience: 30/100\n",
      "Epoch [249/2000], Total Loss: 421.2293, Change Loss: 420.8457, Valid Loss: 0.0017, Vol Loss: 0.1876\n",
      "Val - Total Loss: 446.7768, Change Loss: 446.5049, Valid Loss: 0.0000, Vol Loss: 0.1360\n",
      "Validation loss did not improve. Patience: 31/100\n",
      "Epoch [251/2000], Total Loss: 418.1154, Change Loss: 417.7437, Valid Loss: 0.0019, Vol Loss: 0.1812\n",
      "Val - Total Loss: 446.7171, Change Loss: 446.4449, Valid Loss: 0.0000, Vol Loss: 0.1361\n",
      "Validation loss did not improve. Patience: 33/100\n",
      "Epoch [252/2000], Total Loss: 417.8056, Change Loss: 417.4214, Valid Loss: 0.0018, Vol Loss: 0.1877\n",
      "Val - Total Loss: 441.8496, Change Loss: 441.5767, Valid Loss: 0.0000, Vol Loss: 0.1365\n",
      "Validation loss did not improve. Patience: 34/100\n",
      "Epoch [253/2000], Total Loss: 416.8884, Change Loss: 416.4932, Valid Loss: 0.0022, Vol Loss: 0.1922\n",
      "Val - Total Loss: 455.1621, Change Loss: 454.9009, Valid Loss: 0.0000, Vol Loss: 0.1306\n",
      "Validation loss did not improve. Patience: 35/100\n",
      "Epoch [254/2000], Total Loss: 417.2187, Change Loss: 416.8561, Valid Loss: 0.0019, Vol Loss: 0.1766\n",
      "Val - Total Loss: 444.3590, Change Loss: 444.1051, Valid Loss: 0.0000, Vol Loss: 0.1270\n",
      "Validation loss did not improve. Patience: 36/100\n",
      "Epoch [255/2000], Total Loss: 417.8779, Change Loss: 417.5084, Valid Loss: 0.0017, Vol Loss: 0.1805\n",
      "Val - Total Loss: 443.5443, Change Loss: 443.2785, Valid Loss: 0.0000, Vol Loss: 0.1329\n",
      "Validation loss did not improve. Patience: 37/100\n",
      "Epoch [256/2000], Total Loss: 416.6861, Change Loss: 416.3019, Valid Loss: 0.0019, Vol Loss: 0.1875\n",
      "Val - Total Loss: 452.7414, Change Loss: 452.4756, Valid Loss: 0.0000, Vol Loss: 0.1329\n",
      "Validation loss did not improve. Patience: 38/100\n",
      "Epoch [258/2000], Total Loss: 418.5465, Change Loss: 418.1725, Valid Loss: 0.0015, Vol Loss: 0.1833\n",
      "Val - Total Loss: 446.3278, Change Loss: 446.0689, Valid Loss: 0.0000, Vol Loss: 0.1294\n",
      "Validation loss did not improve. Patience: 40/100\n",
      "Epoch [259/2000], Total Loss: 419.3932, Change Loss: 419.0151, Valid Loss: 0.0019, Vol Loss: 0.1844\n",
      "Val - Total Loss: 442.4726, Change Loss: 442.2132, Valid Loss: 0.0000, Vol Loss: 0.1297\n",
      "Validation loss did not improve. Patience: 41/100\n",
      "Epoch [260/2000], Total Loss: 416.1418, Change Loss: 415.7798, Valid Loss: 0.0016, Vol Loss: 0.1770\n",
      "Val - Total Loss: 449.4303, Change Loss: 449.1750, Valid Loss: 0.0000, Vol Loss: 0.1276\n",
      "Validation loss did not improve. Patience: 42/100\n",
      "Epoch [261/2000], Total Loss: 416.5671, Change Loss: 416.1875, Valid Loss: 0.0014, Vol Loss: 0.1864\n",
      "Val - Total Loss: 448.4901, Change Loss: 448.2332, Valid Loss: 0.0000, Vol Loss: 0.1285\n",
      "Validation loss did not improve. Patience: 43/100\n",
      "Epoch [262/2000], Total Loss: 416.4762, Change Loss: 416.0943, Valid Loss: 0.0021, Vol Loss: 0.1858\n",
      "Val - Total Loss: 450.7803, Change Loss: 450.5111, Valid Loss: 0.0000, Vol Loss: 0.1346\n",
      "Validation loss did not improve. Patience: 44/100\n",
      "Epoch [263/2000], Total Loss: 416.4998, Change Loss: 416.1316, Valid Loss: 0.0014, Vol Loss: 0.1805\n",
      "Val - Total Loss: 445.8763, Change Loss: 445.6216, Valid Loss: 0.0000, Vol Loss: 0.1273\n",
      "Validation loss did not improve. Patience: 45/100\n",
      "Epoch [264/2000], Total Loss: 416.6110, Change Loss: 416.2541, Valid Loss: 0.0012, Vol Loss: 0.1754\n",
      "Val - Total Loss: 457.3275, Change Loss: 457.0634, Valid Loss: 0.0000, Vol Loss: 0.1320\n",
      "Validation loss did not improve. Patience: 46/100\n",
      "Epoch [265/2000], Total Loss: 415.0134, Change Loss: 414.6366, Valid Loss: 0.0016, Vol Loss: 0.1843\n",
      "Val - Total Loss: 446.8003, Change Loss: 446.5461, Valid Loss: 0.0000, Vol Loss: 0.1271\n",
      "Validation loss did not improve. Patience: 47/100\n",
      "Epoch [266/2000], Total Loss: 417.4112, Change Loss: 417.0403, Valid Loss: 0.0015, Vol Loss: 0.1817\n",
      "Val - Total Loss: 446.1030, Change Loss: 445.8491, Valid Loss: 0.0000, Vol Loss: 0.1270\n",
      "Validation loss did not improve. Patience: 48/100\n",
      "Epoch [267/2000], Total Loss: 419.8411, Change Loss: 419.4706, Valid Loss: 0.0016, Vol Loss: 0.1813\n",
      "Val - Total Loss: 451.5045, Change Loss: 451.2413, Valid Loss: 0.0000, Vol Loss: 0.1316\n",
      "Validation loss did not improve. Patience: 49/100\n",
      "Epoch [268/2000], Total Loss: 419.6442, Change Loss: 419.2667, Valid Loss: 0.0017, Vol Loss: 0.1844\n",
      "Val - Total Loss: 453.1824, Change Loss: 452.9332, Valid Loss: 0.0000, Vol Loss: 0.1246\n",
      "Validation loss did not improve. Patience: 50/100\n",
      "Epoch [269/2000], Total Loss: 415.1768, Change Loss: 414.8162, Valid Loss: 0.0018, Vol Loss: 0.1757\n",
      "Val - Total Loss: 458.1791, Change Loss: 457.9173, Valid Loss: 0.0000, Vol Loss: 0.1309\n",
      "Validation loss did not improve. Patience: 51/100\n",
      "Epoch [270/2000], Total Loss: 416.2381, Change Loss: 415.8699, Valid Loss: 0.0015, Vol Loss: 0.1804\n",
      "Val - Total Loss: 446.0295, Change Loss: 445.7719, Valid Loss: 0.0000, Vol Loss: 0.1288\n",
      "Validation loss did not improve. Patience: 52/100\n",
      "Epoch [271/2000], Total Loss: 415.8156, Change Loss: 415.4365, Valid Loss: 0.0017, Vol Loss: 0.1852\n",
      "Val - Total Loss: 439.9343, Change Loss: 439.6874, Valid Loss: 0.0000, Vol Loss: 0.1234\n",
      "Validation loss did not improve. Patience: 53/100\n",
      "Epoch [273/2000], Total Loss: 418.1359, Change Loss: 417.7721, Valid Loss: 0.0014, Vol Loss: 0.1784\n",
      "Val - Total Loss: 453.2472, Change Loss: 453.0121, Valid Loss: 0.0000, Vol Loss: 0.1175\n",
      "Validation loss did not improve. Patience: 55/100\n",
      "Epoch [274/2000], Total Loss: 418.9958, Change Loss: 418.6415, Valid Loss: 0.0016, Vol Loss: 0.1731\n",
      "Val - Total Loss: 447.8373, Change Loss: 447.5992, Valid Loss: 0.0000, Vol Loss: 0.1191\n",
      "Validation loss did not improve. Patience: 56/100\n",
      "Epoch [275/2000], Total Loss: 415.2157, Change Loss: 414.8621, Valid Loss: 0.0017, Vol Loss: 0.1727\n",
      "Val - Total Loss: 444.9201, Change Loss: 444.6786, Valid Loss: 0.0000, Vol Loss: 0.1208\n",
      "Validation loss did not improve. Patience: 57/100\n",
      "Epoch [276/2000], Total Loss: 418.0615, Change Loss: 417.6976, Valid Loss: 0.0013, Vol Loss: 0.1788\n",
      "Val - Total Loss: 443.7551, Change Loss: 443.5134, Valid Loss: 0.0000, Vol Loss: 0.1208\n",
      "Validation loss did not improve. Patience: 58/100\n",
      "Epoch [277/2000], Total Loss: 416.1378, Change Loss: 415.7687, Valid Loss: 0.0015, Vol Loss: 0.1808\n",
      "Val - Total Loss: 441.1474, Change Loss: 440.8983, Valid Loss: 0.0000, Vol Loss: 0.1245\n",
      "Validation loss did not improve. Patience: 59/100\n",
      "Epoch [278/2000], Total Loss: 418.1643, Change Loss: 417.8085, Valid Loss: 0.0016, Vol Loss: 0.1738\n",
      "Val - Total Loss: 442.3007, Change Loss: 442.0432, Valid Loss: 0.0000, Vol Loss: 0.1288\n",
      "Validation loss did not improve. Patience: 60/100\n",
      "Epoch [279/2000], Total Loss: 416.2855, Change Loss: 415.9159, Valid Loss: 0.0014, Vol Loss: 0.1813\n",
      "Val - Total Loss: 450.0329, Change Loss: 449.7689, Valid Loss: 0.0000, Vol Loss: 0.1320\n",
      "Validation loss did not improve. Patience: 61/100\n",
      "Epoch [280/2000], Total Loss: 414.1836, Change Loss: 413.8083, Valid Loss: 0.0021, Vol Loss: 0.1824\n",
      "Val - Total Loss: 442.3115, Change Loss: 442.0548, Valid Loss: 0.0000, Vol Loss: 0.1284\n",
      "Validation loss did not improve. Patience: 62/100\n",
      "Epoch [281/2000], Total Loss: 415.7905, Change Loss: 415.4200, Valid Loss: 0.0018, Vol Loss: 0.1808\n",
      "Val - Total Loss: 440.9994, Change Loss: 440.7481, Valid Loss: 0.0000, Vol Loss: 0.1256\n",
      "Validation loss did not improve. Patience: 63/100\n",
      "Epoch [282/2000], Total Loss: 415.8056, Change Loss: 415.4204, Valid Loss: 0.0016, Vol Loss: 0.1885\n",
      "Val - Total Loss: 443.5347, Change Loss: 443.2770, Valid Loss: 0.0000, Vol Loss: 0.1289\n",
      "Validation loss did not improve. Patience: 64/100\n",
      "Epoch [283/2000], Total Loss: 415.1085, Change Loss: 414.7528, Valid Loss: 0.0012, Vol Loss: 0.1748\n",
      "Val - Total Loss: 450.1668, Change Loss: 449.9230, Valid Loss: 0.0000, Vol Loss: 0.1219\n",
      "Validation loss did not improve. Patience: 65/100\n",
      "Epoch [285/2000], Total Loss: 415.1576, Change Loss: 414.7774, Valid Loss: 0.0014, Vol Loss: 0.1867\n",
      "Val - Total Loss: 448.6445, Change Loss: 448.3907, Valid Loss: 0.0000, Vol Loss: 0.1269\n",
      "Validation loss did not improve. Patience: 67/100\n",
      "Epoch [286/2000], Total Loss: 415.4024, Change Loss: 415.0498, Valid Loss: 0.0017, Vol Loss: 0.1721\n",
      "Val - Total Loss: 447.1603, Change Loss: 446.9073, Valid Loss: 0.0000, Vol Loss: 0.1265\n",
      "Validation loss did not improve. Patience: 68/100\n",
      "Epoch [287/2000], Total Loss: 416.1301, Change Loss: 415.7624, Valid Loss: 0.0013, Vol Loss: 0.1805\n",
      "Val - Total Loss: 447.6127, Change Loss: 447.3584, Valid Loss: 0.0000, Vol Loss: 0.1272\n",
      "Validation loss did not improve. Patience: 69/100\n",
      "Epoch [288/2000], Total Loss: 417.4329, Change Loss: 417.0766, Valid Loss: 0.0015, Vol Loss: 0.1743\n",
      "Val - Total Loss: 440.6841, Change Loss: 440.4413, Valid Loss: 0.0000, Vol Loss: 0.1214\n",
      "Validation loss did not improve. Patience: 70/100\n",
      "Epoch [289/2000], Total Loss: 415.9386, Change Loss: 415.5739, Valid Loss: 0.0014, Vol Loss: 0.1789\n",
      "Val - Total Loss: 448.8389, Change Loss: 448.5440, Valid Loss: 0.0000, Vol Loss: 0.1475\n",
      "Validation loss did not improve. Patience: 71/100\n",
      "Epoch [290/2000], Total Loss: 414.9315, Change Loss: 414.5730, Valid Loss: 0.0012, Vol Loss: 0.1761\n",
      "Val - Total Loss: 448.0794, Change Loss: 447.8325, Valid Loss: 0.0000, Vol Loss: 0.1235\n",
      "Validation loss did not improve. Patience: 72/100\n",
      "Epoch [291/2000], Total Loss: 413.4925, Change Loss: 413.1165, Valid Loss: 0.0017, Vol Loss: 0.1838\n",
      "Val - Total Loss: 448.1493, Change Loss: 447.9027, Valid Loss: 0.0000, Vol Loss: 0.1233\n",
      "Validation loss did not improve. Patience: 73/100\n",
      "Epoch [292/2000], Total Loss: 416.1181, Change Loss: 415.7319, Valid Loss: 0.0015, Vol Loss: 0.1894\n",
      "Val - Total Loss: 451.3673, Change Loss: 451.1209, Valid Loss: 0.0000, Vol Loss: 0.1232\n",
      "Validation loss did not improve. Patience: 74/100\n",
      "Epoch [293/2000], Total Loss: 414.2453, Change Loss: 413.8800, Valid Loss: 0.0016, Vol Loss: 0.1786\n",
      "Val - Total Loss: 444.5604, Change Loss: 444.3234, Valid Loss: 0.0000, Vol Loss: 0.1185\n",
      "Validation loss did not improve. Patience: 75/100\n",
      "Epoch [295/2000], Total Loss: 414.7280, Change Loss: 414.3653, Valid Loss: 0.0009, Vol Loss: 0.1790\n",
      "Val - Total Loss: 447.6292, Change Loss: 447.3882, Valid Loss: 0.0000, Vol Loss: 0.1205\n",
      "Validation loss did not improve. Patience: 77/100\n",
      "Epoch [296/2000], Total Loss: 413.4995, Change Loss: 413.1270, Valid Loss: 0.0013, Vol Loss: 0.1831\n",
      "Val - Total Loss: 447.3568, Change Loss: 447.1066, Valid Loss: 0.0000, Vol Loss: 0.1251\n",
      "Validation loss did not improve. Patience: 78/100\n",
      "Epoch [298/2000], Total Loss: 416.4577, Change Loss: 416.0864, Valid Loss: 0.0014, Vol Loss: 0.1821\n",
      "Val - Total Loss: 451.2542, Change Loss: 451.0068, Valid Loss: 0.0000, Vol Loss: 0.1237\n",
      "Validation loss did not improve. Patience: 80/100\n",
      "Epoch [299/2000], Total Loss: 414.7383, Change Loss: 414.3831, Valid Loss: 0.0010, Vol Loss: 0.1751\n",
      "Val - Total Loss: 445.9920, Change Loss: 445.7481, Valid Loss: 0.0000, Vol Loss: 0.1220\n",
      "Validation loss did not improve. Patience: 81/100\n",
      "Epoch [300/2000], Total Loss: 416.7241, Change Loss: 416.3430, Valid Loss: 0.0014, Vol Loss: 0.1872\n",
      "Val - Total Loss: 459.3122, Change Loss: 459.0584, Valid Loss: 0.0000, Vol Loss: 0.1269\n",
      "Validation loss did not improve. Patience: 82/100\n",
      "Epoch [301/2000], Total Loss: 413.9265, Change Loss: 413.5574, Valid Loss: 0.0015, Vol Loss: 0.1808\n",
      "Val - Total Loss: 452.5527, Change Loss: 452.3017, Valid Loss: 0.0000, Vol Loss: 0.1255\n",
      "Validation loss did not improve. Patience: 83/100\n",
      "Epoch [302/2000], Total Loss: 415.6746, Change Loss: 415.3053, Valid Loss: 0.0013, Vol Loss: 0.1813\n",
      "Val - Total Loss: 445.7943, Change Loss: 445.5352, Valid Loss: 0.0000, Vol Loss: 0.1295\n",
      "Validation loss did not improve. Patience: 84/100\n",
      "Epoch [303/2000], Total Loss: 413.1378, Change Loss: 412.7675, Valid Loss: 0.0014, Vol Loss: 0.1815\n",
      "Val - Total Loss: 451.7461, Change Loss: 451.5018, Valid Loss: 0.0000, Vol Loss: 0.1222\n",
      "Validation loss did not improve. Patience: 85/100\n",
      "Epoch [304/2000], Total Loss: 411.8179, Change Loss: 411.4467, Valid Loss: 0.0015, Vol Loss: 0.1818\n",
      "Val - Total Loss: 449.0365, Change Loss: 448.7808, Valid Loss: 0.0000, Vol Loss: 0.1278\n",
      "Validation loss did not improve. Patience: 86/100\n",
      "Epoch [305/2000], Total Loss: 413.6540, Change Loss: 413.3041, Valid Loss: 0.0014, Vol Loss: 0.1714\n",
      "Val - Total Loss: 445.7819, Change Loss: 445.5309, Valid Loss: 0.0000, Vol Loss: 0.1255\n",
      "Validation loss did not improve. Patience: 87/100\n",
      "Epoch [306/2000], Total Loss: 412.6542, Change Loss: 412.2908, Valid Loss: 0.0011, Vol Loss: 0.1790\n",
      "Val - Total Loss: 442.3716, Change Loss: 442.1256, Valid Loss: 0.0000, Vol Loss: 0.1230\n",
      "Validation loss did not improve. Patience: 88/100\n",
      "Epoch [307/2000], Total Loss: 413.2887, Change Loss: 412.9251, Valid Loss: 0.0009, Vol Loss: 0.1794\n",
      "Val - Total Loss: 448.5592, Change Loss: 448.3088, Valid Loss: 0.0000, Vol Loss: 0.1252\n",
      "Validation loss did not improve. Patience: 89/100\n",
      "Epoch [308/2000], Total Loss: 417.0906, Change Loss: 416.7070, Valid Loss: 0.0012, Vol Loss: 0.1887\n",
      "Val - Total Loss: 453.2879, Change Loss: 453.0372, Valid Loss: 0.0000, Vol Loss: 0.1254\n",
      "Validation loss did not improve. Patience: 90/100\n",
      "Epoch [309/2000], Total Loss: 414.1933, Change Loss: 413.8319, Valid Loss: 0.0011, Vol Loss: 0.1779\n",
      "Val - Total Loss: 452.1169, Change Loss: 451.8654, Valid Loss: 0.0000, Vol Loss: 0.1258\n",
      "Validation loss did not improve. Patience: 91/100\n",
      "Epoch [310/2000], Total Loss: 414.1382, Change Loss: 413.7985, Valid Loss: 0.0014, Vol Loss: 0.1663\n",
      "Val - Total Loss: 449.6149, Change Loss: 449.3630, Valid Loss: 0.0000, Vol Loss: 0.1259\n",
      "Validation loss did not improve. Patience: 92/100\n",
      "Epoch [311/2000], Total Loss: 412.3893, Change Loss: 412.0219, Valid Loss: 0.0009, Vol Loss: 0.1815\n",
      "Val - Total Loss: 444.7819, Change Loss: 444.5278, Valid Loss: 0.0000, Vol Loss: 0.1271\n",
      "Validation loss did not improve. Patience: 93/100\n",
      "Epoch [312/2000], Total Loss: 414.5757, Change Loss: 414.2019, Valid Loss: 0.0012, Vol Loss: 0.1839\n",
      "Val - Total Loss: 454.5647, Change Loss: 454.3027, Valid Loss: 0.0000, Vol Loss: 0.1310\n",
      "Validation loss did not improve. Patience: 94/100\n",
      "Epoch [313/2000], Total Loss: 414.3468, Change Loss: 413.9834, Valid Loss: 0.0015, Vol Loss: 0.1780\n",
      "Val - Total Loss: 440.4917, Change Loss: 440.2249, Valid Loss: 0.0000, Vol Loss: 0.1333\n",
      "Validation loss did not improve. Patience: 95/100\n",
      "Epoch [314/2000], Total Loss: 412.7232, Change Loss: 412.3611, Valid Loss: 0.0010, Vol Loss: 0.1787\n",
      "Val - Total Loss: 439.6814, Change Loss: 439.4129, Valid Loss: 0.0000, Vol Loss: 0.1343\n",
      "Validation loss did not improve. Patience: 96/100\n",
      "Epoch [315/2000], Total Loss: 410.3198, Change Loss: 409.9435, Valid Loss: 0.0014, Vol Loss: 0.1847\n",
      "Val - Total Loss: 438.3007, Change Loss: 438.0285, Valid Loss: 0.0000, Vol Loss: 0.1361\n",
      "Validation loss did not improve. Patience: 97/100\n",
      "Epoch [317/2000], Total Loss: 416.1647, Change Loss: 415.7921, Valid Loss: 0.0012, Vol Loss: 0.1833\n",
      "Val - Total Loss: 440.6289, Change Loss: 440.3603, Valid Loss: 0.0000, Vol Loss: 0.1343\n",
      "Validation loss did not improve. Patience: 99/100\n",
      "Epoch [318/2000], Total Loss: 414.4268, Change Loss: 414.0683, Valid Loss: 0.0012, Vol Loss: 0.1763\n",
      "Val - Total Loss: 446.7226, Change Loss: 446.4540, Valid Loss: 0.0000, Vol Loss: 0.1343\n",
      "Validation loss did not improve. Patience: 100/100\n",
      "Early stopping triggered after 318 epochs. Best validation loss: 429.4459\n",
      "\n",
      "预测结果 (复原后的价格):\n",
      "[[ 11.61        11.61        11.3301525   11.347009    -2.6953106\n",
      "   -3.48663      0.12457968  -0.13797915]\n",
      " [ 11.347009    11.347009    11.107659    11.1295     -15.048252\n",
      "  -15.672083     0.12933213  -0.12960424]\n",
      " [ 11.1295      11.1295      10.895685    10.911674   -17.285196\n",
      "  -17.92664      0.12876451  -0.1436052 ]\n",
      " [ 10.911674    10.911674    10.6730995   10.687799   -15.838433\n",
      "  -16.512316     0.12805718  -0.14356972]\n",
      " [ 10.687799    10.687799    10.457115    10.47158    -16.510971\n",
      "  -17.1627       0.12789917  -0.14211066]\n",
      " [ 10.47158     10.47158     10.246397    10.260452   -16.60304\n",
      "  -17.249022     0.1276404   -0.14435934]\n",
      " [ 10.260452    10.260452    10.038744    10.052223   -16.441282\n",
      "  -17.085981     0.12736075  -0.1456542 ]]\n",
      "\n",
      "实际结果 (原始数据):\n",
      "[[Timestamp('2018-02-23 00:00:00') 'sh.600000' '12.6600' '12.7800'\n",
      "  '12.5900' '12.7200' '12.6300' '38736515' '491768409.0000' '3'\n",
      "  '0.137834' '1' '0.712590' '0']\n",
      " [Timestamp('2018-02-26 00:00:00') 'sh.600000' '12.7700' '12.8400'\n",
      "  '12.6100' '12.7300' '12.7200' '48806055' '620261080.0000' '3'\n",
      "  '0.173664' '1' '0.078611' '0']\n",
      " [Timestamp('2018-02-27 00:00:00') 'sh.600000' '12.7500' '12.7600'\n",
      "  '12.6100' '12.6900' '12.7300' '48561087' '615622149.0000' '3'\n",
      "  '0.172792' '1' '-0.314218' '0']\n",
      " [Timestamp('2018-02-28 00:00:00') 'sh.600000' '12.6100' '12.6600'\n",
      "  '12.4400' '12.4600' '12.6900' '39888938' '499258844.0000' '3'\n",
      "  '0.141935' '1' '-1.812447' '0']\n",
      " [Timestamp('2018-03-01 00:00:00') 'sh.600000' '12.4000' '12.5000'\n",
      "  '12.3800' '12.4700' '12.4600' '34491031' '429574072.0000' '3'\n",
      "  '0.122727' '1' '0.080259' '0']\n",
      " [Timestamp('2018-03-02 00:00:00') 'sh.600000' '12.4000' '12.4600'\n",
      "  '12.3600' '12.4100' '12.4700' '23024132' '285473130.0000' '3'\n",
      "  '0.081925' '1' '-0.481158' '0']\n",
      " [Timestamp('2018-03-05 00:00:00') 'sh.600000' '12.4100' '12.4800'\n",
      "  '12.3400' '12.4200' '12.4100' '29343464' '363849705.0000' '3'\n",
      "  '0.104411' '1' '0.080582' '0']]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测K线图:\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected (7,8), got (7, 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 703\u001b[39m\n\u001b[32m    701\u001b[39m plot_kline(predictions.cpu().numpy(), start_date=start_date)\n\u001b[32m    702\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m预测K线图:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m \u001b[43mplot_kline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m实际K线图:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 592\u001b[39m, in \u001b[36mplot_kline\u001b[39m\u001b[34m(predictions, start_date)\u001b[39m\n\u001b[32m    588\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    589\u001b[39m \u001b[33;03m绘制7天K线图\u001b[39;00m\n\u001b[32m    590\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    591\u001b[39m \u001b[38;5;66;03m# 验证输入\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m predictions.shape == (\u001b[32m7\u001b[39m, \u001b[32m8\u001b[39m), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected (7,8), got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictions.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    594\u001b[39m ohlc = predictions[:, :\u001b[32m4\u001b[39m]  \u001b[38;5;66;03m# open, high, low, close\u001b[39;00m\n\u001b[32m    595\u001b[39m volume = predictions[:, \u001b[32m4\u001b[39m]  \u001b[38;5;66;03m# volume\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: Expected (7,8), got (7, 14)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import numpy as np\n",
    "import baostock as bs\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib.dates import DateFormatter, DayLocator, date2num\n",
    "# pip install baostock mplfinance\n",
    "\n",
    "# 配置参数\n",
    "SEED = 42\n",
    "LOOKBACK = 30\n",
    "FORECAST = 7\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 2\n",
    "D_MODEL = 256\n",
    "D_FF = 512\n",
    "DROPOUT = 0.5\n",
    "LR = 0.0001\n",
    "EPOCHS = 2000\n",
    "PATIENCE = 100\n",
    "\n",
    "# 损失函数权重 - 现在更简单，因为预测变化率自然会产生波动\n",
    "val_coef = 1.0    # 最终价格准确性\n",
    "vaild_coef = 5.0  # 价格合法性\n",
    "vol_coef = 2.0    # 波动率匹配\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 设置随机种子\n",
    "torch.random.manual_seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(DEVICE)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Transformer组件\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_features, d_model):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.linear = nn.Linear(num_features, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        encoding = encoding.unsqueeze(0)\n",
    "        self.register_buffer('encoding', encoding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1), :]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.drop(attn_weights)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_len = x.size(0), x.size(1), x.size(2)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x.contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.combine_heads(attn_output)\n",
    "        output = self.W_o(output)\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.activation = nn.GELU()\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.self_attn(x, x, x)\n",
    "        x = self.drop(x)\n",
    "        x = self.norm1(x + residual)\n",
    "        residual = x\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.norm2(x + residual)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_feature, num_layers, d_model, num_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(num_feature, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.drop(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encode_ouput, mask):\n",
    "        residual = x\n",
    "        x = self.self_attn(x, x, x, mask)\n",
    "        x = self.drop(x)\n",
    "        x = self.norm1(x + residual)\n",
    "        residual = x\n",
    "        x = self.enc_dec_attn(x, encode_ouput, encode_ouput)\n",
    "        x = self.drop(x)\n",
    "        x = self.norm2(x + residual)\n",
    "        residual = x\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.norm3(x + residual)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_feature, num_layers, d_model, num_heads, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = Embedding(num_feature, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d_model, num_feature)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.drop(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, mask)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def create_look_ahead_mask(size, device=None):\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1).type(torch.uint8)\n",
    "    mask = mask == 0\n",
    "    if device is not None:\n",
    "        mask = mask.to(device)\n",
    "    return mask\n",
    "\n",
    "def validity_loss(x):\n",
    "    \"\"\"\n",
    "    计算预测结果的合法性损失\n",
    "    \"\"\"\n",
    "    high = x[:, :, 1]  # 最高价变化率\n",
    "    low = x[:, :, 2]   # 最低价变化率\n",
    "    open_price = x[:, :, 0]  # 开盘价变化率\n",
    "    close = x[:, :, 3]  # 收盘价变化率\n",
    "\n",
    "    # 1. 检查价格变化是否合理（高价变化不应小于低价变化）\n",
    "    low_high_violation = torch.relu(low - high)\n",
    "    \n",
    "    # 2. 开盘价和收盘价应该在最低价和最高价变化范围内\n",
    "    open_bounds_violation = torch.relu(open_price - high) + torch.relu(low - open_price)\n",
    "    close_bounds_violation = torch.relu(close - high) + torch.relu(low - close)\n",
    "    \n",
    "    # 3. 修正：价格变化不应过大（防止极端预测）\n",
    "    # 取4个价格特征中变化最大的那个（绝对值）\n",
    "    max_change = torch.max(torch.abs(x[:, :, :4]), dim=2)[0]\n",
    "    # 使用50表示50%的变化限制（因为数据是百分比形式）\n",
    "    extreme_change = torch.relu(max_change - 50.0)  \n",
    "\n",
    "    total_violation = (\n",
    "        10.0 * low_high_violation +\n",
    "        5.0 * open_bounds_violation +\n",
    "        5.0 * close_bounds_violation +\n",
    "        3.0 * extreme_change\n",
    "    )\n",
    "    \n",
    "    return total_violation.mean()\n",
    "\n",
    "def infer(encoder, decoder, inputs, original_df, start_idx, infer_steps):\n",
    "    \"\"\"\n",
    "    inputs: [1, seq_len, num_features]\n",
    "    original_df: 原始DataFrame用于获取基准价格\n",
    "    start_idx: 输入序列在原始数据中的起始索引\n",
    "    results: [infer_steps, num_features]\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # 获取模型所在的设备\n",
    "    device = next(encoder.parameters()).device\n",
    "    \n",
    "    # 确保输入数据在正确的设备上\n",
    "    inputs = inputs.to(device)\n",
    "    results = []\n",
    "    \n",
    "    # 获取基准价格（输入序列的最后一天的原始价格）\n",
    "    base_idx = start_idx + inputs.size(1) - 1\n",
    "    base_prices = original_df.iloc[base_idx][['open', 'high', 'low', 'close']].values.astype(float)\n",
    "    base_open = base_prices[0]\n",
    "    base_high = base_prices[1]\n",
    "    base_low = base_prices[2]\n",
    "    base_close = base_prices[3]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        shift_outputs = inputs[:, -1:, :].clone()\n",
    "        \n",
    "        encode = encoder(inputs)\n",
    "        for step in range(infer_steps):\n",
    "            mask_size = shift_outputs.size(1)\n",
    "            mask = create_look_ahead_mask(mask_size, device)\n",
    "            \n",
    "            decode = decoder(shift_outputs, encode, mask)\n",
    "            \n",
    "            # 获取预测的变化率\n",
    "            pred_changes = decode[:, -1, :].cpu().numpy()[0]\n",
    "            \n",
    "            # 复原实际价格\n",
    "            if step == 0:\n",
    "                # 第一天使用基准价格\n",
    "                pred_open = base_open\n",
    "            else:\n",
    "                # 后续天使用前一天的收盘价\n",
    "                pred_open = results[-1][3]\n",
    "            \n",
    "            # 计算当天的OHLC\n",
    "            pred_high = pred_open * (1 + pred_changes[1] / 100)\n",
    "            pred_low = pred_open * (1 + pred_changes[2] / 100)\n",
    "            pred_close = pred_open * (1 + pred_changes[3] / 100)\n",
    "            \n",
    "            # 确保价格关系合理\n",
    "            pred_high = max(pred_high, pred_open, pred_close)\n",
    "            pred_low = min(pred_low, pred_open, pred_close)\n",
    "            \n",
    "            # 保存复原后的价格\n",
    "            restored = np.array([\n",
    "                pred_open, pred_high, pred_low, pred_close,\n",
    "                *pred_changes[4:]  # 其他特征直接使用预测值\n",
    "            ])\n",
    "            \n",
    "            results.append(restored)\n",
    "            \n",
    "            # 为下一次预测准备输入\n",
    "            # 创建新的变化率输入（相对于前一天）\n",
    "            if step == 0:\n",
    "                # 第一天的变化率是相对于基准日\n",
    "                new_input = np.zeros_like(pred_changes)\n",
    "                new_input[0] = (pred_open - base_open) / base_open * 100\n",
    "                new_input[1] = (pred_high - base_open) / base_open * 100\n",
    "                new_input[2] = (pred_low - base_open) / base_open * 100\n",
    "                new_input[3] = (pred_close - base_open) / base_open * 100\n",
    "                new_input[4:] = pred_changes[4:]\n",
    "            else:\n",
    "                # 后续天的变化率是相对于前一天\n",
    "                prev_open = results[-2][0]\n",
    "                new_input = np.zeros_like(pred_changes)\n",
    "                new_input[0] = (pred_open - prev_open) / prev_open * 100\n",
    "                new_input[1] = (pred_high - prev_open) / prev_open * 100\n",
    "                new_input[2] = (pred_low - prev_open) / prev_open * 100\n",
    "                new_input[3] = (pred_close - prev_open) / prev_open * 100\n",
    "                new_input[4:] = pred_changes[4:]\n",
    "            \n",
    "            # 添加到shift_outputs\n",
    "            new_input_tensor = torch.tensor(new_input, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)\n",
    "            shift_outputs = torch.cat((shift_outputs, new_input_tensor), dim=1)\n",
    "\n",
    "    return torch.tensor(np.array(results), device=device, dtype=torch.float32)\n",
    "\n",
    "def prepare_data(data, lookback, forecast):\n",
    "    \"\"\"\n",
    "    准备训练数据\n",
    "    注意：data已经是变化率数据，不是原始价格\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    # 从lookback开始，确保有足够历史数据\n",
    "    for i in range(lookback, len(data) - forecast):\n",
    "        # X是lookback天的变化率序列\n",
    "        X.append(data[i-lookback+1:i+1])\n",
    "        # y是接下来forecast天的变化率\n",
    "        y.append(data[i+1:i+1+forecast])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def fetch_data_loader():\n",
    "    bs.login()\n",
    "    rs = bs.query_history_k_data_plus(\"sh.600000\",\n",
    "        \"date,code,open,high,low,close,preclose,volume,amount,adjustflag,turn,tradestatus,pctChg,isST\",\n",
    "        start_date='2018-01-01', end_date='2024-12-31',\n",
    "        frequency=\"d\", adjustflag=\"3\")\n",
    "    \n",
    "    data_list = []\n",
    "    while (rs.error_code == '0') & rs.next():\n",
    "        data_list.append(rs.get_row_data())\n",
    "    \n",
    "    df = pd.DataFrame(data_list, columns=rs.fields)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values(by='date')\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # 保存日期列用于后续绘图\n",
    "    date_series = df['date'].copy()\n",
    "    \n",
    "    # 保留'date'列在original_df中，但不用于模型输入\n",
    "    original_df = df.copy()\n",
    "    \n",
    "    # 用于模型的特征列（不包含'date'）\n",
    "    feature_columns = ['open', 'high', 'low', 'close', 'volume', 'amount', 'turn', 'pctChg']\n",
    "    df = df[feature_columns]\n",
    "    df = df.astype(float)\n",
    "    \n",
    "    print(\"原始数据前5行:\")\n",
    "    print(df.head())\n",
    "\n",
    "    bs.logout()\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    \n",
    "    # 关键修改：计算价格变化率而不是使用绝对价格\n",
    "    data = df.values\n",
    "    transformed_data = np.zeros_like(data)\n",
    "    \n",
    "    # 保留第一天的原始数据作为基准\n",
    "    transformed_data[0] = data[0]\n",
    "    \n",
    "    # 计算每天相对于前一天的变化率（百分比）\n",
    "    for i in range(1, len(data)):\n",
    "        # 价格列: 计算变化率 (相对变化)\n",
    "        for j in [0, 1, 2, 3]:  # open, high, low, close\n",
    "            transformed_data[i, j] = (data[i, j] - data[i-1, j]) / data[i-1, j] * 100\n",
    "        \n",
    "        # 成交量列: 计算变化率\n",
    "        for j in [4, 5]:  # volume, amount\n",
    "            transformed_data[i, j] = (data[i, j] - data[i-1, j]) / (data[i-1, j] + 1e-8) * 100\n",
    "        \n",
    "        # 换手率和涨跌幅直接使用\n",
    "        transformed_data[i, 6] = data[i, 6]  # turn\n",
    "        transformed_data[i, 7] = data[i, 7]  # pctChg\n",
    "    \n",
    "    # 填充第一天的涨跌幅为0 (没有前一天数据)\n",
    "    transformed_data[0, 7] = 0\n",
    "    \n",
    "    print(\"\\n变化率数据前5行:\")\n",
    "    print(transformed_data[:5])\n",
    "    \n",
    "    # 准备训练数据\n",
    "    X, y = prepare_data(transformed_data, LOOKBACK, FORECAST)\n",
    "    \n",
    "    # 划分训练集和验证集\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED, shuffle=False)\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, original_df, date_series\n",
    "\n",
    "def get_loss(predicted, target):\n",
    "    \"\"\"简化损失函数 - 预测变化率自然会产生波动\"\"\"\n",
    "    # 基本损失：预测变化率与目标变化率的差异\n",
    "    change_loss = nn.functional.mse_loss(predicted, target)\n",
    "    \n",
    "    # 价格合法性损失\n",
    "    vaild_loss = validity_loss(predicted)\n",
    "    \n",
    "    # 波动率匹配损失\n",
    "    pred_close = predicted[:, :, 3]  # 预测的收盘价变化率\n",
    "    target_close = target[:, :, 3]   # 目标收盘价变化率\n",
    "    \n",
    "    # 计算预测和目标的波动率\n",
    "    pred_volatility = torch.std(pred_close, dim=1)\n",
    "    target_volatility = torch.std(target_close, dim=1)\n",
    "    \n",
    "    # Huber损失匹配波动率\n",
    "    vol_loss = nn.functional.huber_loss(pred_volatility, target_volatility, reduction='mean')\n",
    "    \n",
    "    # 组合损失\n",
    "    total_loss = (val_coef * change_loss +\n",
    "                  vaild_coef * vaild_loss +\n",
    "                  vol_coef * vol_loss)\n",
    "    \n",
    "    return total_loss, change_loss, vaild_loss, vol_loss\n",
    "\n",
    "def train(encoder, decoder, train_loader, val_loader, lr, epochs, patience):\n",
    "    \"\"\"训练函数\"\"\"\n",
    "    encoder.to(DEVICE)\n",
    "    decoder.to(DEVICE)\n",
    "    \n",
    "    params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=lr)\n",
    "    \n",
    "    # 早停机制相关变量\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = {\n",
    "        'encoder': None,\n",
    "        'decoder': None,\n",
    "        'epoch': -1,\n",
    "        'val_loss': float('inf')\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        train_loss = 0\n",
    "        train_change_loss = 0\n",
    "        train_vaild_loss = 0\n",
    "        train_vol_loss = 0\n",
    "        \n",
    "        for train_x, train_y in train_loader:\n",
    "            train_x, train_y = train_x.to(DEVICE), train_y.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            enc_output = encoder(train_x)\n",
    "            \n",
    "            mask_size = train_y.size(1)\n",
    "            mask = create_look_ahead_mask(mask_size, DEVICE)\n",
    "            \n",
    "            decoder_input = torch.cat([train_x[:, -1:, :], train_y[:, :-1, :]], dim=1)\n",
    "            \n",
    "            dec_output = decoder(decoder_input, enc_output, mask)\n",
    "            \n",
    "            # 组合损失\n",
    "            loss, change_loss, vaild_loss, vol_loss = get_loss(dec_output, train_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # 添加梯度裁剪，防止梯度爆炸\n",
    "            torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_change_loss += change_loss.item()\n",
    "            train_vaild_loss += vaild_loss.item()\n",
    "            train_vol_loss += vol_loss.item()\n",
    "        \n",
    "        train_avg_loss = train_loss / len(train_loader)\n",
    "        train_avg_change_loss = train_change_loss / len(train_loader)\n",
    "        train_avg_vaild_loss = train_vaild_loss / len(train_loader)\n",
    "        train_avg_vol_loss = train_vol_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Total Loss: {train_avg_loss:.4f}, Change Loss: {train_avg_change_loss:.4f}, \"\n",
    "              f\"Valid Loss: {train_avg_vaild_loss:.4f}, Vol Loss: {train_avg_vol_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        val_loss = 0\n",
    "        val_change_loss = 0\n",
    "        val_vaild_loss = 0\n",
    "        val_vol_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_x, val_y in val_loader:\n",
    "                val_x, val_y = val_x.to(DEVICE), val_y.to(DEVICE)\n",
    "                \n",
    "                enc_output = encoder(val_x)\n",
    "                \n",
    "                mask_size = val_y.size(1)\n",
    "                mask = create_look_ahead_mask(mask_size, DEVICE)\n",
    "                \n",
    "                decoder_input = torch.cat([val_x[:, -1:, :], val_y[:, :-1, :]], dim=1)\n",
    "                \n",
    "                dec_output = decoder(decoder_input, enc_output, mask)\n",
    "                \n",
    "                # 组合损失\n",
    "                loss, change_loss, vaild_loss, vol_loss = get_loss(dec_output, val_y)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_change_loss += change_loss.item()\n",
    "                val_vaild_loss += vaild_loss.item()\n",
    "                val_vol_loss += vol_loss.item()\n",
    "                \n",
    "        val_avg_loss = val_loss / len(val_loader)\n",
    "        val_avg_change_loss = val_change_loss / len(val_loader)\n",
    "        val_avg_vaild_loss = val_vaild_loss / len(val_loader)\n",
    "        val_avg_vol_loss = val_vol_loss / len(val_loader)\n",
    "        print(f\"Val - Total Loss: {val_avg_loss:.4f}, Change Loss: {val_avg_change_loss:.4f}, \"\n",
    "              f\"Valid Loss: {val_avg_vaild_loss:.4f}, Vol Loss: {val_avg_vol_loss:.4f}\")\n",
    "        \n",
    "        # 早停机制检查\n",
    "        if val_avg_loss < best_val_loss:\n",
    "            best_val_loss = val_avg_loss\n",
    "            patience_counter = 0\n",
    "            # 保存最佳模型状态\n",
    "            best_model_state = {\n",
    "                'encoder': encoder.state_dict().copy(),\n",
    "                'decoder': decoder.state_dict().copy(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': val_avg_loss\n",
    "            }\n",
    "            print(f\"Validation loss improved to {val_avg_loss:.4f}, saving model...\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation loss did not improve. Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        # 检查是否应该早停\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs. Best validation loss: {best_val_loss:.4f}\")\n",
    "            # 恢复最佳模型\n",
    "            encoder.load_state_dict(best_model_state['encoder'])\n",
    "            decoder.load_state_dict(best_model_state['decoder'])\n",
    "            break\n",
    "    \n",
    "    # 如果没有早停，确保最后保存的是最佳模型\n",
    "    if patience_counter < patience:\n",
    "        encoder.load_state_dict(best_model_state['encoder'])\n",
    "        decoder.load_state_dict(best_model_state['decoder'])\n",
    "        print(f\"Training completed. Best validation loss: {best_val_loss:.4f} at epoch {best_model_state['epoch']+1}\")\n",
    "\n",
    "def plot_kline(predictions, start_date=None):\n",
    "    \"\"\"\n",
    "    绘制7天K线图\n",
    "    \"\"\"\n",
    "    # 验证输入\n",
    "    assert predictions.shape == (7, 8), f\"Expected (7,8), got {predictions.shape}\"\n",
    "    \n",
    "    ohlc = predictions[:, :4]  # open, high, low, close\n",
    "    volume = predictions[:, 4]  # volume\n",
    "    \n",
    "    # 处理日期\n",
    "    if start_date is None:\n",
    "        start_date = datetime.now()\n",
    "    else:\n",
    "        start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    # 只保留日期，去除时间部分\n",
    "    start_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    dates = [start_date + timedelta(days=i) for i in range(7)]\n",
    "    \n",
    "    EPS = 1e-5  # 容差，防止浮点误差误报\n",
    "    \n",
    "    for i in range(7):\n",
    "        o, h, l, c = ohlc[i]\n",
    "        \n",
    "        if h < l - EPS:\n",
    "            print(f\"警告: high < low [H={h:.4f}, L={l:.4f}]\")\n",
    "        elif o < l - EPS or o > h + EPS:\n",
    "            print(f\"警告: open 超出 [low, high] 范围 [O={o:.4f}, L={l:.4f}, H={h:.4f}]\")\n",
    "        elif c < l - EPS or c > h + EPS:\n",
    "            print(f\"警告: close 超出 [low, high] 范围 [C={c:.4f}, L={l:.4f}, H={h:.4f}]\")\n",
    "    \n",
    "    # 准备 OHLC 数据\n",
    "    ohlc_data = [\n",
    "        [date2num(date), o, h, l, c]\n",
    "        for date, (o, h, l, c) in zip(dates, ohlc)\n",
    "    ]\n",
    "    \n",
    "    # 创建子图\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), gridspec_kw={'height_ratios': [3, 1]})\n",
    "    \n",
    "    # 绘制 K 线\n",
    "    candlestick_ohlc(ax1, ohlc_data, width=0.6, colorup='red', colordown='green')\n",
    "    ax1.set_title('7-Day Stock Price Prediction', fontsize=14)\n",
    "    ax1.set_ylabel('price')\n",
    "    ax1.xaxis.set_major_formatter(DateFormatter('%m-%d'))\n",
    "    ax1.xaxis.set_major_locator(DayLocator())\n",
    "    ax1.grid(True, axis='y', alpha=0.3)\n",
    "    \n",
    "    # 旋转标签\n",
    "    for label in ax1.xaxis.get_majorticklabels():\n",
    "        label.set_rotation(45)\n",
    "    \n",
    "    # 绘制成交量\n",
    "    ax2.bar([date2num(date) for date in dates], volume, width=0.6, color='gray', alpha=0.7)\n",
    "    ax2.set_ylabel('Volume')\n",
    "    ax2.xaxis.set_major_formatter(DateFormatter('%m-%d'))\n",
    "    ax2.xaxis.set_major_locator(DayLocator())\n",
    "    ax2.grid(True, axis='y', alpha=0.3)\n",
    "    \n",
    "    for label in ax2.xaxis.get_majorticklabels():\n",
    "        label.set_rotation(45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 在训练前添加设备检查\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "# 获取数据 - 现在返回date_series\n",
    "train_loader, val_loader, original_df, date_series = fetch_data_loader()\n",
    "\n",
    "# 获取特征数量\n",
    "num_features = train_loader.dataset[0][0].shape[-1]\n",
    "\n",
    "# 初始化模型\n",
    "encoder = Encoder(num_features, NUM_LAYERS, D_MODEL, NUM_HEADS, D_FF, DROPOUT)\n",
    "decoder = Decoder(num_features, NUM_LAYERS, D_MODEL, NUM_HEADS, D_FF, DROPOUT)\n",
    "\n",
    "# 训练模型\n",
    "train(encoder, decoder, train_loader, val_loader, lr=LR, epochs=EPOCHS, patience=PATIENCE)\n",
    "\n",
    "# 示例推理\n",
    "sample_idx = 3\n",
    "sample_input, sample_output = val_loader.dataset[sample_idx]\n",
    "sample_input = sample_input.unsqueeze(0)  # 添加批次维度\n",
    "\n",
    "# 计算原始数据中的起始索引\n",
    "start_idx = sample_idx + LOOKBACK - 1\n",
    "\n",
    "# 确保模型在正确的设备上\n",
    "encoder.to(DEVICE)\n",
    "decoder.to(DEVICE)\n",
    "sample_input = sample_input.to(DEVICE)\n",
    "\n",
    "# 使用修改后的推理函数\n",
    "predictions = infer(encoder, decoder, sample_input, original_df, start_idx, FORECAST)\n",
    "\n",
    "# 获取对应的原始目标数据（用于比较）\n",
    "target_data = original_df.iloc[start_idx+1:start_idx+1+FORECAST].values\n",
    "\n",
    "# 获取开始日期（从date_series中获取）\n",
    "start_date = date_series.iloc[start_idx+1].strftime('%Y-%m-%d')\n",
    "\n",
    "# 打印预测结果\n",
    "print(\"\\n预测结果 (复原后的价格):\")\n",
    "print(predictions.cpu().numpy())\n",
    "\n",
    "print(\"\\n实际结果 (原始数据):\")\n",
    "print(target_data)\n",
    "\n",
    "# 绘制K线图\n",
    "plot_kline(predictions.cpu().numpy(), start_date=start_date)\n",
    "print(\"预测K线图:\")\n",
    "plot_kline(target_data, start_date=start_date)\n",
    "print(\"实际K线图:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
